{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e5a098",
   "metadata": {},
   "source": [
    "# This is a full pipeline with keypoints prediction using VAE in reconstruction mode for VoxCeleb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4af7e9",
   "metadata": {},
   "source": [
    "# Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "afa4a0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os   \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "import numpy as np\n",
    "import imageio\n",
    "from Training_Prediction.FOMM.Source_Model.sync_batchnorm import DataParallelWithCallback\n",
    "from Training_Prediction.PREDICTOR.VAE import VAE_origin\n",
    "from Training_Prediction.FOMM.Source_Model.augmentation import SelectRandomFrames, SelectFirstFrames_two, VideoToTensor\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Training_Prediction.FOMM.Source_Model.frames_dataset import FramesDataset\n",
    "import tensorflow.compat.v1 as tf\n",
    "import pickle\n",
    "from Training_Prediction.PREDICTOR.Source_Model.prediction_toplevel import KPDataset,get_data_from_dataloader_60\n",
    "import gc\n",
    "import pickle\n",
    "import yaml\n",
    "from Training_Prediction.FOMM.Source_Model.modules.generator import OcclusionAwareGenerator,calculate_frechet_distance,compute_fvd\n",
    "from Training_Prediction.FOMM.Source_Model.modules.keypoint_detector import KPDetector\n",
    "#from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer, Visualizer_slow\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "from torch import nn\n",
    "import tensorflow.compat.v1 as tf\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import vmaf\n",
    "import subprocess\n",
    "import imageio\n",
    "import json\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "import shutil\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "b8915fe4-5e75-4773-8495-a34bbea2ea5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_png(frames, png_dir):\n",
    "    for idx, frame in enumerate(frames):\n",
    "        frame_path = os.path.join(png_dir, f'video_frame_{idx:04d}.png')\n",
    "        imageio.imsave(frame_path, (255 * frame).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "fc7cf7d3-18db-4f9a-bb32-ba513cf73090",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_png_to_mp4(png_dir,mp4_dir,video_number):\n",
    "    if not png_dir.endswith('/'):\n",
    "        png_dir += '/'\n",
    "    if not mp4_dir.endswith('/'):\n",
    "        mp4_dir += '/'\n",
    "    \n",
    "    input_pattern = f\"{png_dir}video_frame_%04d.png\"\n",
    "    output_file = f\"{mp4_dir}video_{video_number:02d}.mp4\"\n",
    "    command = (f'ffmpeg -y -r 60 -i {png_dir}/video_frame_%04d.png -pix_fmt yuv420p -profile:v high -level:v 4.1 -crf:v 20 -movflags +faststart {mp4_dir}/video_{video_number:02d}.mp4')\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "68291432-b39e-4d3d-b199-e7c2799d982b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_png_to_y4m(png_dir, y4m_dir, video_number):\n",
    "    if not png_dir.endswith('/'):\n",
    "        png_dir += '/'\n",
    "    if not y4m_dir.endswith('/'):\n",
    "        y4m_dir += '/'\n",
    "    \n",
    "    input_pattern = f\"{png_dir}video_frame_%04d.png\"\n",
    "    output_file = f\"{y4m_dir}video_{video_number:02d}.y4m\"\n",
    "    \n",
    "    command = (f'ffmpeg -y -i {input_pattern} -r 25 -pix_fmt yuv444p {output_file}')    \n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "99b12791-2ab7-410e-bc58-304a1ffebeed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vmaf(driving_vid, generated_vid, output_path):\n",
    "    if driving_vid[-3:] != 'y4m':\n",
    "         raise Exception('Video must be in y4m format.')\n",
    "    if generated_vid[-3:] != 'y4m':\n",
    "         raise Exception('Video must be in y4m format.')\n",
    "    if output_path[-4:] != 'json':\n",
    "        raise Exception('Output file must be in json format')\n",
    "    \n",
    "    command = (f'vmaf --reference {driving_vid} --distorted {generated_vid} --model version=vmaf_v0.6.1 --output {output_path} --json')\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "763d131c",
   "metadata": {},
   "source": [
    "# Define VAE model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8cc260a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lag_time = 12\n",
    "\n",
    "\n",
    "model = VAE_origin(60, lag_time=lag_time,\n",
    "          hidden_size=256, hidden_layer_depth=3,\n",
    "          batch_size=250, n_epochs=100, cuda=True, \n",
    "          sliding_window=True, dropout_rate=0.3,\n",
    "          learning_rate=1E-3, scale=1E-3, autocorr=False, encoder_size=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f25b2a",
   "metadata": {},
   "source": [
    "# After VAE is trained, load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0b6477d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VDE(input_size=60, encoder_size=20, n_epochs=100,\n",
       "    batch_size=250, lag_time=12, sliding_window=True,\n",
       "    autocorr=False, cuda=True)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('Checkpoints/VAE_3883videos_vox_12-12.pth')) # 12-12 frames\n",
    "# model.load_state_dict(torch.load('Checkpoints/VAE_3883videos_vox_6-6.pth')) # 6-6 frames\n",
    "\n",
    "# Set the model to evaluation mode (important if using dropout or batch normalization)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67981bd7",
   "metadata": {},
   "source": [
    "# Import keypoints of 44 VoxCeleb test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90335bc9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"kp_test_44_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "len(kp_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7088199",
   "metadata": {},
   "source": [
    "# Convert list of keypoints to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ebeed979",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa46fe1b",
   "metadata": {},
   "source": [
    "# Apply min-max std to keypoints and convert to batches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2333a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "(118, 60)\n"
     ]
    }
   ],
   "source": [
    "kp_list_test = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_test.append(kp_one_video_array)\n",
    "    \n",
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_test_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_test)):\n",
    "    data = kp_list_test[video_idx]\n",
    "    data_length = len(kp_list_test[video_idx])\n",
    "    step_interval = 12 # choose between 12 frames or 24 frames \n",
    "    min_required_steps = 2*step_interval\n",
    "    selected_data = []\n",
    "    for i in range(0, data_length - min_required_steps+1, 2 * step_interval):\n",
    "        selected_data.extend(data[i:i + step_interval])\n",
    "    min_values = np.min(selected_data,axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(selected_data,axis=0) # 60 maxs of one selected video in the loop \n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_test[video_idx] - min_values) / range_values\n",
    "    kp_list_test_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_test_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ad330665",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset batches: 529\n",
      "(24, 60)\n"
     ]
    }
   ],
   "source": [
    "######### convert into batches:\n",
    "frames = min_required_steps\n",
    "input_frames = int(frames / 2)\n",
    "data_batch_test = []\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_test.append(arr)\n",
    "print(f'test dataset batches:', len(data_batch_test))\n",
    "print(data_batch_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "eff75c1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### test dataset:\n",
    "\n",
    "test_data_reshape = np.array(data_batch_test).reshape(-1,frames,60)\n",
    "test_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c19e5-ee36-44cc-a8c7-7caff90e7d29",
   "metadata": {},
   "source": [
    "# Predict keypoints using trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "20c9c688-c031-4a26-8a24-2720d2b21e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([529, 12, 60])\n",
      "torch.Size([529, 12, 60])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_30843/207256912.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  validation_input = torch.tensor(validation_input).to(device)\n"
     ]
    }
   ],
   "source": [
    "# test dataset\n",
    "validation_data = test_data_reshape\n",
    "\n",
    "# evaluate model:\n",
    "validation_input = torch.tensor(validation_data[:,:input_frames], dtype = torch.float32) # input: [24,10,17]\n",
    "kp_gt = torch.tensor(validation_data[:,input_frames:], dtype = torch.float32) # gtoundtruth: [24,10,17]\n",
    "\n",
    "# Move the model to the desired device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.to(device)\n",
    "\n",
    "# Check the device of your input tensor (x)\n",
    "validation_input = torch.tensor(validation_input).to(device)\n",
    "\n",
    "pred_encoded = model.encoder(validation_input)\n",
    "pred_lmbd = model.lmbd(pred_encoded)\n",
    "pred = model.decoder(pred_lmbd)\n",
    "\n",
    "print(kp_gt.shape)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b86c4-b761-48d9-ad9a-88908129633b",
   "metadata": {},
   "source": [
    "# Generate unstd keypoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "dd71d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches of each video: 44\n"
     ]
    }
   ],
   "source": [
    "# save num_batches for each video:\n",
    "num_batch_video = []\n",
    "num_full_batches_all = 0\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] > frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        num_full_batches_all += num_full_batches\n",
    "        num_batch_video.append(num_full_batches)\n",
    "print(f'number of batches of each video:', len(num_batch_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "dacf886e-0203-4086-bedb-4468e1cf4fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first half of frames: groundtruth; last half of frames: predicted\n",
    "test_gt_pred = np.concatenate((test_data_reshape[:,:input_frames], pred.detach().cpu()), axis = 1)\n",
    "\n",
    "test_gt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "0f220aff-17ab-43e2-a335-3b54857cc1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#for no prediction\n",
    "test_video_unstd_list = kp_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "e56be0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstd for each video -- for prediction\n",
    "test_video_unstd_list = []\n",
    "for video_idx in range(len(num_batch_video)):\n",
    "    test_video = test_gt_pred[sum(num_batch_video[:video_idx]):sum(num_batch_video[:video_idx+1])]\n",
    "    test_video_unstd = test_video * range_list[video_idx] + min_list[video_idx]\n",
    "    test_video_unstd_list.append(test_video_unstd) # unstd video keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74a397-20c6-4395-b7b9-c14de9621770",
   "metadata": {},
   "source": [
    "# Optical flow and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fee04637",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use predefined train-test split.\n",
      "using videos from test directory\n",
      "['id10280#NXjT3732Ekg#001093#001192.mp4', 'id10281#NHARUN9OhSo#000605#000886.mp4', 'id10281#NHARUN9OhSo#001059#001210.mp4', 'id10281#NHARUN9OhSo#002098#002175.mp4', 'id10281#NHARUN9OhSo#002209#002570.mp4', 'id10281#NHARUN9OhSo#006609#006906.mp4', 'id10281#NHARUN9OhSo#006912#007284.mp4', 'id10281#NHARUN9OhSo#007425#007663.mp4', 'id10282#IDA_ElNHLn4#000674#000852.mp4', 'id10282#IDA_ElNHLn4#001226#001390.mp4', 'id10283#N69Hp2DGMLk#000519#000619.mp4', 'id10283#N69Hp2DGMLk#000721#000842.mp4', 'id10283#N69Hp2DGMLk#000893#001589.mp4', 'id10283#N69Hp2DGMLk#004133#005157.mp4', 'id10283#N69Hp2DGMLk#005157#005316.mp4', 'id10283#N69Hp2DGMLk#005931#006184.mp4', 'id10283#N69Hp2DGMLk#006184#006353.mp4', 'id10283#N69Hp2DGMLk#006405#006583.mp4', 'id10283#N69Hp2DGMLk#006600#007118.mp4', 'id10283#N69Hp2DGMLk#007129#007281.mp4', 'id10283#r9-0pljhZqs#002414#002769.mp4', 'id10283#r9-0pljhZqs#003725#003847.mp4', 'id10283#r9-0pljhZqs#004062#004408.mp4', 'id10283#r9-0pljhZqs#007271#007498.mp4', 'id10283#r9-0pljhZqs#007920#008172.mp4', 'id10283#r9-0pljhZqs#008373#008488.mp4', 'id10283#r9-0pljhZqs#009636#009808.mp4', 'id10283#r9-0pljhZqs#010121#010605.mp4', 'id10283#r9-0pljhZqs#010765#010954.mp4', 'id10283#r9-0pljhZqs#011158#011299.mp4', 'id10283#r9-0pljhZqs#012307#012561.mp4', 'id10283#r9-0pljhZqs#013264#013592.mp4', 'id10283#r9-0pljhZqs#013743#013963.mp4', 'id10283#r9-0pljhZqs#014353#014609.mp4', 'id10283#r9-0pljhZqs#014722#014831.mp4', 'id10283#r9-0pljhZqs#014864#015079.mp4', 'id10283#r9-0pljhZqs#015364#015539.mp4', 'id10283#rUnfAxF1dJI#001389#001843.mp4', 'id10283#uA1E_38TuTw#001699#001863.mp4', 'id10283#uA1E_38TuTw#002329#002480.mp4', 'id10283#uA1E_38TuTw#002875#002990.mp4', 'id10283#uA1E_38TuTw#003095#003227.mp4', 'id10283#uA1E_38TuTw#003246#003371.mp4', 'id10283#x6M3KQ8-gM4#000018#000441.mp4']\n",
      "44\n"
     ]
    }
   ],
   "source": [
    "####### call the config functions and inference dataloader #########\n",
    "config=\"config/abs-vox.yml\"\n",
    "\n",
    "# Test dataset\n",
    "with open(config) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "dataset = FramesDataset(is_train=(False), **config['dataset_params'],mode=\"RNN\") # test\n",
    "\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "### call the functions        \n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                            **config['model_params']['common_params'])\n",
    "\n",
    "log_dir=\"./log/test-reconstruction-vox\"\n",
    "checkpoint=\"./Training_Prediction/FOMM/Trained_Models/vox-cpk.pth.tar\"\n",
    "\n",
    "if checkpoint is not None:\n",
    "    Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
    "else:\n",
    "    raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "pred_png_dir = os.path.join(log_dir, 'prediction/prediction_png')\n",
    "pred_mp4_dir = os.path.join(log_dir, 'prediction/prediction_mp4')\n",
    "pred_y4m_dir = os.path.join(log_dir, 'prediction/prediction_y4m')\n",
    "driving_png_dir = os.path.join(log_dir, 'prediction/driving_png')\n",
    "driving_mp4_dir = os.path.join(log_dir, 'prediction/driving_mp4')\n",
    "driving_y4m_dir = os.path.join(log_dir, 'prediction/driving_y4m')\n",
    "vae_vmaf_output = os.path.join(log_dir, 'prediction/vmaf_json_output_vae')\n",
    "\n",
    "log_dir = os.path.join(log_dir, 'prediction')\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(pred_png_dir):\n",
    "    os.makedirs(pred_png_dir)\n",
    "if not os.path.exists(pred_mp4_dir):\n",
    "    os.makedirs(pred_mp4_dir)\n",
    "if not os.path.exists(pred_y4m_dir):\n",
    "    os.makedirs(pred_y4m_dir)\n",
    "\n",
    "if not os.path.exists(driving_png_dir):\n",
    "    os.makedirs(driving_png_dir)\n",
    "if not os.path.exists(driving_mp4_dir):\n",
    "    os.makedirs(driving_mp4_dir)\n",
    "if not os.path.exists(driving_y4m_dir):\n",
    "    os.makedirs(driving_y4m_dir)\n",
    "\n",
    "if not os.path.exists(vae_vmaf_output):\n",
    "    os.makedirs(vae_vmaf_output)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator = DataParallelWithCallback(generator)\n",
    "    kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "generator.eval()\n",
    "kp_detector.eval()\n",
    "\n",
    "prediction_params = config['prediction_params']\n",
    "\n",
    "num_epochs = prediction_params['num_epochs']\n",
    "lr = prediction_params['lr']\n",
    "bs = prediction_params['batch_size']\n",
    "num_frames = prediction_params['num_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "104215c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [02:39,  3.63s/it]\n"
     ]
    }
   ],
   "source": [
    "#########  FOMM+VAE ########\n",
    "mse_list_videos = []\n",
    "for it, x in tqdm(enumerate(dataloader)):\n",
    "        if config['reconstruction_params']['num_videos'] is not None:\n",
    "            if it > config['reconstruction_params']['num_videos']:\n",
    "                break\n",
    "    \n",
    "        # Clear the PNG directories for each iteration of the outer loop\n",
    "        shutil.rmtree(pred_png_dir, ignore_errors=True)\n",
    "        os.makedirs(pred_png_dir)\n",
    "    \n",
    "        shutil.rmtree(driving_png_dir, ignore_errors=True)\n",
    "        os.makedirs(driving_png_dir)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            driving_frames = []\n",
    "            \n",
    "            ######## keypoints ########\n",
    "            kp_driving_video = test_video_unstd_list[it].reshape(-1,10,6)\n",
    "            kp_driving_video = torch.tensor(kp_driving_video)\n",
    "            kp_source = {\"value\":kp_driving_video[0,:,:2].reshape(1,10,2),\"jacobian\":kp_driving_video[0,:,2:].reshape(1,10,2,2)} # kp of the ith frame      \n",
    "        mse_list_frames = []\n",
    "        ##### Start generator\n",
    "        for i in range(((x['video'].shape[2])//frames)*frames): # cut the last <24 frames\n",
    "            source = x['video'][:, :, 0]\n",
    "            driving = x['video'][:, :, i]\n",
    "            kp_driving = {\"value\":kp_driving_video[i,:,:2],\"jacobian\":kp_driving_video[i,:,2:]} # kp of the ith frame\n",
    "            kp_driving['value'] = kp_driving['value'].reshape(1,10,2)\n",
    "            kp_driving['jacobian'] = kp_driving['jacobian'].reshape(1,10,2,2)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "            out['kp_source'] = kp_source\n",
    "            out['kp_driving'] = kp_driving\n",
    "            del out['sparse_deformed']\n",
    "            driving_frames.append(np.transpose(driving.data.cpu().numpy(),[0, 2, 3, 1])[0])\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "            mse_list_frames.append(np.mean(np.square(driving.cpu().numpy() - out['prediction'].detach().cpu().numpy())))\n",
    "#        predictions = np.concatenate(predictions, axis=1)\n",
    "        mse_list_videos.append(mse_list_frames)\n",
    "\n",
    "        #Save frames of predicted and driving videos as png's\n",
    "#        save_frames_as_png(predictions, pred_png_dir)\n",
    "#        save_frames_as_png(driving_frames,driving_png_dir)\n",
    "\n",
    "        #Convert frames into a y4m-format video\n",
    "#        convert_png_to_y4m(pred_png_dir, pred_y4m_dir, it)\n",
    "#        convert_png_to_y4m(driving_png_dir, driving_y4m_dir, it)\n",
    "\n",
    "        #Run VMAF on y4m-format videos\n",
    "#        run_vmaf(driving_y4m_dir+f'/video_{it:02d}.y4m',pred_y4m_dir+f'/video_{it:02d}.y4m',vae_vmaf_output+f'/vmaf_{it:02d}.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8e0efca-a8d0-4de3-ad0d-9bf84ca2a62d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Read all VMAF outputs into a list variable\n",
    "vmaf_outputs = []\n",
    "for output_num in range(0,44):\n",
    "    with open(vae_vmaf_output + f'/vmaf_{output_num:02d}.json','r') as file:\n",
    "        vmaf_outputs.append(json.load(file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "40d7fcd4-3323-44ba-bd0d-284f58862b3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get all per-video VMAF scores \n",
    "vmaf_scores = []\n",
    "for score in vmaf_outputs:\n",
    "    vmaf_scores.append(score['pooled_metrics']['vmaf']['mean'])\n",
    "#10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "df01c078-59ce-4eab-812b-bb67dff5fb2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Take one video as an example of per-frame scores\n",
    "vmaf_example_video = vmaf_outputs[10]['frames']\n",
    "frame_vmaf_scores = []\n",
    "for frame in vmaf_example_video:\n",
    "    frame_vmaf_scores.append(frame['metrics']['vmaf'])\n",
    "mse_example_video = mse_list_videos[10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "237fe4d2-59e4-4082-9685-5ed179784a49",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(vmaf_example_video)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
