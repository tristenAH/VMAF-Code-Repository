{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e5a098",
   "metadata": {},
   "source": [
    "# This is a full pipeline with keypoints prediction using VRNN in reconstruction mode for VoxCeleb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4af7e9",
   "metadata": {},
   "source": [
    "# Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa4a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-12 02:01:13.648356: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-12 02:01:14.218799: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "import numpy as np\n",
    "import imageio\n",
    "from Training_Prediction.FOMM.Source_Model.sync_batchnorm import DataParallelWithCallback\n",
    "from Training_Prediction.FOMM.Source_Model.modules.RNN_prediction_module import PredictionModule\n",
    "from Training_Prediction.FOMM.Source_Model.augmentation import SelectRandomFrames, SelectFirstFrames_two, VideoToTensor\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Training_Prediction.FOMM.Source_Model.frames_dataset import FramesDataset\n",
    "import tensorflow.compat.v1 as tf\n",
    "from Training_Prediction.PREDICTOR.Source_Model.VRNN import build_vrnn, get_config\n",
    "import pickle\n",
    "from Training_Prediction.PREDICTOR.Source_Model.VRNN_prediction import VRNN_predict\n",
    "from Training_Prediction.PREDICTOR.Source_Model.prediction_toplevel import KPDataset,get_data_from_dataloader_60\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "import yaml\n",
    "from Training_Prediction.FOMM.Source_Model.modules.generator import OcclusionAwareGenerator,calculate_frechet_distance,compute_fvd\n",
    "from Training_Prediction.FOMM.Source_Model.modules.keypoint_detector import KPDetector\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer, Visualizer_slow\n",
    "from torch import nn\n",
    "\n",
    "import tensorflow.compat.v1 as tf\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b914babd",
   "metadata": {},
   "source": [
    "# Import keypoints of 3883 training videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75dddb4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "UnpicklingError",
     "evalue": "pickle data was truncated",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnpicklingError\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mkp_train_3883_vox.pkl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 2\u001b[0m     kp_time_series \u001b[38;5;241m=\u001b[39m \u001b[43mpickle\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28mlen\u001b[39m(kp_time_series)\n",
      "\u001b[0;31mUnpicklingError\u001b[0m: pickle data was truncated"
     ]
    }
   ],
   "source": [
    "with open(\"kp_train_3883_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "len(kp_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6229644",
   "metadata": {},
   "source": [
    "# Convert list of keypoints to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "546511ff",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kp_time_series' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# convert list of keypoints to dictionary: \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m video_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[43mkp_time_series\u001b[49m)):\n\u001b[1;32m      3\u001b[0m     kp_time_series[video_idx] \u001b[38;5;241m=\u001b[39m kp_time_series[video_idx][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkp\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m      5\u001b[0m kp_dict_init \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kp_time_series' is not defined"
     ]
    }
   ],
   "source": [
    "# convert list of keypoints to dictionary: \n",
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5abf8684",
   "metadata": {},
   "source": [
    "# Apply min-max standardization to keypoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebb966a",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_list_train = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_train.append(kp_one_video_array)\n",
    "    \n",
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_train_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_train)):\n",
    "    min_values = np.min(kp_list_train[video_idx],axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(kp_list_train[video_idx],axis=0) # 60 maxs of one selected video in the loop\n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_train[video_idx] - min_values) / range_values\n",
    "    kp_list_train_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_train_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e42c77e0",
   "metadata": {},
   "source": [
    "# Convert standardized keypoints to mini-batches: 12 or 24 frames a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba0edb3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### convert data into batches #########\n",
    "data_batch_train = []\n",
    "\n",
    "frames = 24 # 24 as one batch, use 12 ground truth frames as input to predict next 12 frames as output\n",
    "input_frames = int(frames / 2)\n",
    "input_dim = 60\n",
    "for t,x in enumerate(kp_list_train_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_train.append(arr)\n",
    "print(f'train dataset batches:', len(data_batch_train))\n",
    "print(data_batch_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fc174b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### train dataset:\n",
    "\n",
    "train_data_reshape = np.array(data_batch_train).reshape(-1,frames,60)\n",
    "train_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e345974b",
   "metadata": {},
   "source": [
    "#  Train VRNN with build_vrnn_new/build_vrnn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63ae4d2e-1a4e-4398-8349-d8dd6fda01aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_vrnn\n",
    "\n",
    "print(\"using VRNN\")\n",
    "train_data = train_data_reshape\n",
    "\n",
    "frames = 24\n",
    "data_all = {}\n",
    "data_all['value'] = []\n",
    "data_all['jacobian'] = []\n",
    "data_lee = []\n",
    "\n",
    "#data_lee = get_data_50(test_dataset)\n",
    "data_lee = train_data\n",
    "print(data_lee.shape)\n",
    "data_lee = np.reshape(data_lee,(-1,frames,10,6))\n",
    "print(data_lee.shape)\n",
    "data_lee = tf.convert_to_tensor(data_lee)\n",
    "print(data_lee.shape)\n",
    "cfg = get_config()\n",
    "input_keypoint = tf.keras.Input(shape=[frames,10,6],name='keypoint')\n",
    "observed_keypoints_stop = tf.keras.layers.Lambda(tf.stop_gradient)(\n",
    "input_keypoint)\n",
    "vrnn_model = model(cfg)\n",
    "predicted_keypoints, kl_divergence = vrnn_model(observed_keypoints_stop)\n",
    "train_model = tf.keras.Model(inputs=[input_keypoint],outputs=[predicted_keypoints])\n",
    "vrnn_coord_pred_loss = tf.nn.l2_loss(\n",
    "observed_keypoints_stop - predicted_keypoints)\n",
    "# Normalize by batch size and sequence length:\n",
    "vrnn_coord_pred_loss /= tf.to_float(\n",
    "  tf.shape(input_keypoint)[0] * tf.shape(input_keypoint)[1])\n",
    "train_model.add_loss(vrnn_coord_pred_loss)\n",
    "kl_loss = tf.reduce_mean(kl_divergence)  # Mean over batch and timesteps.\n",
    "train_model.add_loss(cfg.kl_loss_scale * kl_loss)\n",
    "vrnn_optimizer = tf.keras.optimizers.Adam(lr=cfg.learning_rate, clipnorm = cfg.clipnorm)\n",
    "train_model.compile(vrnn_optimizer)\n",
    "# Specify the checkpoint path\n",
    "checkpoint_path = \"Checkpoints/VRNN_3883videos_vox_12-12.ckpt\" # 15-15 frames\n",
    "\n",
    "checkpoint_dir = os.path.dirname(checkpoint_path)\n",
    "\n",
    "# Create a callback to save the model's weights\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(filepath=checkpoint_path,\n",
    "                                             save_weights_only=True,\n",
    "                                             verbose=1)\n",
    "\n",
    "print(\"Training VRNN_LEE prediction...\")\n",
    "train_model.fit(x=data_lee, steps_per_epoch = train_data.shape[0], epochs=10, callbacks=[cp_callback]) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050fe1a-6b18-42f6-b9c5-c797ab8fe3ee",
   "metadata": {},
   "source": [
    "# After VRNN is trained, load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "631fc218-f893-4e25-a74a-15333bfa0666",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_vrnn\n",
    "frames = 24\n",
    "cfg = get_config()\n",
    "input_keypoint = tf.keras.Input(shape=[frames,10,6],name='keypoint')\n",
    "observed_keypoints_stop = tf.keras.layers.Lambda(tf.stop_gradient)(\n",
    "input_keypoint)\n",
    "vrnn_model = model(cfg)\n",
    "predicted_keypoints, kl_divergence = vrnn_model(observed_keypoints_stop)\n",
    "train_model = tf.keras.Model(inputs=[input_keypoint],outputs=[predicted_keypoints])\n",
    "vrnn_coord_pred_loss = tf.nn.l2_loss(\n",
    "observed_keypoints_stop - predicted_keypoints)\n",
    "# Normalize by batch size and sequence length:\n",
    "vrnn_coord_pred_loss /= tf.to_float(\n",
    "  tf.shape(input_keypoint)[0] * tf.shape(input_keypoint)[1])\n",
    "train_model.add_loss(vrnn_coord_pred_loss)\n",
    "kl_loss = tf.reduce_mean(kl_divergence)  # Mean over batch and timesteps.\n",
    "train_model.add_loss(cfg.kl_loss_scale * kl_loss)\n",
    "\n",
    "# Load saved model:\n",
    "cfg = get_config()\n",
    "checkpoint_path = \"Checkpoints/VRNN_3883videos_vox_12-12.ckpt\" \n",
    "# checkpoint_path = \"Checkpoints/VRNN_3883videos_vox_6-6.ckpt\" \n",
    "\n",
    "# Loads the weights\n",
    "train_model.load_weights(checkpoint_path)\n",
    "train_model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8be408d6",
   "metadata": {},
   "source": [
    "# Import keypoints of 44 VoxCeleb test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94cbe1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kp_test_44_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "len(kp_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fdc9b27",
   "metadata": {},
   "source": [
    "# Convert list of keypoints to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "95da1f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90273494",
   "metadata": {},
   "source": [
    "# Apply min-max std to keypoints and convert to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "075a2afb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "(118, 60)\n"
     ]
    }
   ],
   "source": [
    "kp_list_test = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_test.append(kp_one_video_array)\n",
    "    \n",
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_test_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_test)):\n",
    "    data = kp_list_test[video_idx]\n",
    "    data_length = len(kp_list_test[video_idx])\n",
    "    step_interval = 12 # choose between 12 frames or 24 frames \n",
    "    min_required_steps = 2*step_interval\n",
    "    selected_data = []\n",
    "    for i in range(0, data_length - min_required_steps+1, 2 * step_interval):\n",
    "        selected_data.extend(data[i:i + step_interval])\n",
    "    min_values = np.min(selected_data,axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(selected_data,axis=0) # 60 maxs of one selected video in the loop \n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_test[video_idx] - min_values) / range_values\n",
    "    kp_list_test_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_test_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7f9de646",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset batches: 529\n",
      "(24, 60)\n"
     ]
    }
   ],
   "source": [
    "######### convert into batches:\n",
    "frames = min_required_steps\n",
    "input_frames = int(frames / 2)\n",
    "data_batch_test = []\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_test.append(arr)\n",
    "print(f'test dataset batches:', len(data_batch_test))\n",
    "print(data_batch_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8872f207",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### test dataset:\n",
    "\n",
    "test_data_reshape = np.array(data_batch_test).reshape(-1,frames,60)\n",
    "test_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c19e5-ee36-44cc-a8c7-7caff90e7d29",
   "metadata": {},
   "source": [
    "# Predict keypoints using trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "20c9c688-c031-4a26-8a24-2720d2b21e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 9/17 [==============>...............] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-08 01:15:25.323035: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 5s 15ms/step\n",
      "(529, 24, 10, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset and process model.predict():\n",
    "\n",
    "validation_data = test_data_reshape\n",
    "\n",
    "validation_data_tensor = tf.convert_to_tensor(validation_data.reshape(-1,frames,10,6))\n",
    "pred = train_model.predict(validation_data_tensor)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b86c4-b761-48d9-ad9a-88908129633b",
   "metadata": {},
   "source": [
    "# Generate unstd keypoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "27d67c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches of each video: 44\n"
     ]
    }
   ],
   "source": [
    "# save num_batches for each video:\n",
    "num_batch_video = []\n",
    "num_full_batches_all = 0\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        num_full_batches_all += num_full_batches\n",
    "        num_batch_video.append(num_full_batches)\n",
    "print(f'number of batches of each video:', len(num_batch_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dacf886e-0203-4086-bedb-4468e1cf4fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first half of frames: groundtruth; last half of frames: predicted\n",
    "test_gt_pred = np.concatenate((test_data_reshape[:,:input_frames], pred.reshape(-1,frames,60)[:,input_frames:]), axis = 1)\n",
    "test_gt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc63c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstd for each video:\n",
    "test_video_unstd_list = []\n",
    "for video_idx in range(len(num_batch_video)):\n",
    "    test_video = test_gt_pred[sum(num_batch_video[:video_idx]):sum(num_batch_video[:video_idx+1])]\n",
    "    test_video_unstd = test_video * range_list[video_idx] + min_list[video_idx]\n",
    "    test_video_unstd_list.append(test_video_unstd) # unstd video keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74a397-20c6-4395-b7b9-c14de9621770",
   "metadata": {},
   "source": [
    "# Optical flow and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "938a6ee0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use predefined train-test split.\n",
      "using videos from test directory\n",
      "['id10280#NXjT3732Ekg#001093#001192.mp4', 'id10281#NHARUN9OhSo#000605#000886.mp4', 'id10281#NHARUN9OhSo#001059#001210.mp4', 'id10281#NHARUN9OhSo#002098#002175.mp4', 'id10281#NHARUN9OhSo#002209#002570.mp4', 'id10281#NHARUN9OhSo#006609#006906.mp4', 'id10281#NHARUN9OhSo#006912#007284.mp4', 'id10281#NHARUN9OhSo#007425#007663.mp4', 'id10282#IDA_ElNHLn4#000674#000852.mp4', 'id10282#IDA_ElNHLn4#001226#001390.mp4', 'id10283#N69Hp2DGMLk#000519#000619.mp4', 'id10283#N69Hp2DGMLk#000721#000842.mp4', 'id10283#N69Hp2DGMLk#000893#001589.mp4', 'id10283#N69Hp2DGMLk#004133#005157.mp4', 'id10283#N69Hp2DGMLk#005157#005316.mp4', 'id10283#N69Hp2DGMLk#005931#006184.mp4', 'id10283#N69Hp2DGMLk#006184#006353.mp4', 'id10283#N69Hp2DGMLk#006405#006583.mp4', 'id10283#N69Hp2DGMLk#006600#007118.mp4', 'id10283#N69Hp2DGMLk#007129#007281.mp4', 'id10283#r9-0pljhZqs#002414#002769.mp4', 'id10283#r9-0pljhZqs#003725#003847.mp4', 'id10283#r9-0pljhZqs#004062#004408.mp4', 'id10283#r9-0pljhZqs#007271#007498.mp4', 'id10283#r9-0pljhZqs#007920#008172.mp4', 'id10283#r9-0pljhZqs#008373#008488.mp4', 'id10283#r9-0pljhZqs#009636#009808.mp4', 'id10283#r9-0pljhZqs#010121#010605.mp4', 'id10283#r9-0pljhZqs#010765#010954.mp4', 'id10283#r9-0pljhZqs#011158#011299.mp4', 'id10283#r9-0pljhZqs#012307#012561.mp4', 'id10283#r9-0pljhZqs#013264#013592.mp4', 'id10283#r9-0pljhZqs#013743#013963.mp4', 'id10283#r9-0pljhZqs#014353#014609.mp4', 'id10283#r9-0pljhZqs#014722#014831.mp4', 'id10283#r9-0pljhZqs#014864#015079.mp4', 'id10283#r9-0pljhZqs#015364#015539.mp4', 'id10283#rUnfAxF1dJI#001389#001843.mp4', 'id10283#uA1E_38TuTw#001699#001863.mp4', 'id10283#uA1E_38TuTw#002329#002480.mp4', 'id10283#uA1E_38TuTw#002875#002990.mp4', 'id10283#uA1E_38TuTw#003095#003227.mp4', 'id10283#uA1E_38TuTw#003246#003371.mp4', 'id10283#x6M3KQ8-gM4#000018#000441.mp4']\n",
      "44\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 23.68 GiB total capacity; 992.28 MiB already allocated; 26.69 MiB free; 1.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 22\u001b[0m\n\u001b[1;32m     19\u001b[0m checkpoint\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./Training_Prediction/FOMM/Trained_Models/vox-cpk.pth.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     21\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m checkpoint \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 22\u001b[0m     \u001b[43mLogger\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_cpk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkp_detector\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkp_detector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     24\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCheckpoint should be specified for mode=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreconstruction\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/gen-mod-vol/tristen/Motion-Transfer-Keypoints-Prediction/Keypoints_Prediction/Training_Prediction/FOMM/Source_Model/logger.py:54\u001b[0m, in \u001b[0;36mLogger.load_cpk\u001b[0;34m(checkpoint_path, generator, discriminator, kp_detector, optimizer_generator, optimizer_discriminator, optimizer_kp_detector)\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m     52\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_cpk\u001b[39m(checkpoint_path, generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, discriminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, kp_detector\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     53\u001b[0m              optimizer_generator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, optimizer_discriminator\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, optimizer_kp_detector\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m---> 54\u001b[0m     checkpoint \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcheckpoint_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     55\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m generator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     56\u001b[0m         generator\u001b[38;5;241m.\u001b[39mload_state_dict(checkpoint[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgenerator\u001b[39m\u001b[38;5;124m'\u001b[39m])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:815\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, **pickle_load_args)\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    814\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m pickle\u001b[38;5;241m.\u001b[39mUnpicklingError(UNSAFE_MESSAGE \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mstr\u001b[39m(e)) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 815\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_legacy_load\u001b[49m\u001b[43m(\u001b[49m\u001b[43mopened_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mpickle_load_args\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:1043\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1041\u001b[0m unpickler \u001b[38;5;241m=\u001b[39m UnpicklerWrapper(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1042\u001b[0m unpickler\u001b[38;5;241m.\u001b[39mpersistent_load \u001b[38;5;241m=\u001b[39m persistent_load\n\u001b[0;32m-> 1043\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43munpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1045\u001b[0m deserialized_storage_keys \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mload(f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpickle_load_args)\n\u001b[1;32m   1047\u001b[0m offset \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mtell() \u001b[38;5;28;01mif\u001b[39;00m f_should_read_directly \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:980\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.persistent_load\u001b[0;34m(saved_id)\u001b[0m\n\u001b[1;32m    976\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_torch_load_uninitialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    977\u001b[0m     \u001b[38;5;66;03m# TODO: Once we decide to break serialization FC, we can\u001b[39;00m\n\u001b[1;32m    978\u001b[0m     \u001b[38;5;66;03m# stop wrapping with TypedStorage\u001b[39;00m\n\u001b[1;32m    979\u001b[0m     typed_storage \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mstorage\u001b[38;5;241m.\u001b[39mTypedStorage(\n\u001b[0;32m--> 980\u001b[0m         wrap_storage\u001b[38;5;241m=\u001b[39m\u001b[43mrestore_location\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    981\u001b[0m         dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[1;32m    982\u001b[0m         _internal\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    983\u001b[0m     deserialized_objects[root_key] \u001b[38;5;241m=\u001b[39m typed_storage\n\u001b[1;32m    984\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:217\u001b[0m, in \u001b[0;36mdefault_restore_location\u001b[0;34m(storage, location)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdefault_restore_location\u001b[39m(storage, location):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m _, _, fn \u001b[38;5;129;01min\u001b[39;00m _package_registry:\n\u001b[0;32m--> 217\u001b[0m         result \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstorage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m result \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    219\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/torch/serialization.py:185\u001b[0m, in \u001b[0;36m_cuda_deserialize\u001b[0;34m(obj, location)\u001b[0m\n\u001b[1;32m    183\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(obj, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_torch_load_uninitialized\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice(device):\n\u001b[0;32m--> 185\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mUntypedStorage\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnbytes\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    186\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    187\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mcuda(device)\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 36.00 MiB (GPU 0; 23.68 GiB total capacity; 992.28 MiB already allocated; 26.69 MiB free; 1.11 GiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "####### call the config functions and inference dataloader #########\n",
    "config=\"config/abs-vox.yml\"\n",
    "\n",
    "# Test dataset\n",
    "with open(config) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "dataset = FramesDataset(is_train=(False), **config['dataset_params'],mode=\"RNN\") # test\n",
    "\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "### call the functions        \n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                            **config['model_params']['common_params'])\n",
    "\n",
    "log_dir=\"./log/test-reconstruction-vox\"\n",
    "checkpoint=\"./Training_Prediction/FOMM/Trained_Models/vox-cpk.pth.tar\"\n",
    "\n",
    "if checkpoint is not None:\n",
    "    Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
    "else:\n",
    "    raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "png_dir = os.path.join(log_dir, 'prediction/png')\n",
    "log_dir = os.path.join(log_dir, 'prediction')\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(png_dir):\n",
    "    os.makedirs(png_dir)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator = DataParallelWithCallback(generator)\n",
    "    kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "generator.eval()\n",
    "kp_detector.eval()\n",
    "\n",
    "prediction_params = config['prediction_params']\n",
    "\n",
    "num_epochs = prediction_params['num_epochs']\n",
    "lr = prediction_params['lr']\n",
    "bs = prediction_params['batch_size']\n",
    "num_frames = prediction_params['num_frames']\n",
    "loss_list_total = []\n",
    "fvd_list_total = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "6d8fbeb7-642e-44ba-aa08-1cc77195b23b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Image must be 2D (grayscale, RGB, or RGBA).",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[19], line 37\u001b[0m\n\u001b[1;32m     35\u001b[0m     frame_name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mframe_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.png\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     36\u001b[0m     frame_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(png_dir, frame_name)\n\u001b[0;32m---> 37\u001b[0m     \u001b[43mimageio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimsave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mframe_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m255\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mprediction\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpu\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muint8\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m i \u001b[38;5;241m=\u001b[39m i \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/imageio/v2.py:392\u001b[0m, in \u001b[0;36mimwrite\u001b[0;34m(uri, im, format, **kwargs)\u001b[0m\n\u001b[1;32m    389\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage is not numeric, but \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(imt\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m))\n\u001b[1;32m    391\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_batch(im) \u001b[38;5;129;01mor\u001b[39;00m im\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[0;32m--> 392\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mImage must be 2D (grayscale, RGB, or RGBA).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    394\u001b[0m imopen_args \u001b[38;5;241m=\u001b[39m decypher_format_arg(\u001b[38;5;28mformat\u001b[39m)\n\u001b[1;32m    395\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: Image must be 2D (grayscale, RGB, or RGBA)."
     ]
    }
   ],
   "source": [
    "i = 0\n",
    "for it, x in tqdm(enumerate(dataloader)):\n",
    "    if config['reconstruction_params']['num_videos'] is not None:\n",
    "        if it > config['reconstruction_params']['num_videos']:\n",
    "            break\n",
    "    if i >= 1:\n",
    "        break\n",
    "    # Clear the PNG directory for each iteration of the outer loop\n",
    "    shutil.rmtree(png_dir, ignore_errors=True)\n",
    "    os.makedirs(png_dir)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        visualizations = []\n",
    "\n",
    "        ######## keypoints ########\n",
    "        kp_driving_video = test_video_unstd_list[it].reshape(-1, 10, 6)\n",
    "        kp_driving_video = torch.tensor(kp_driving_video)\n",
    "        kp_source = {\"value\":kp_driving_video[0,:,:2].reshape(1,10,2),\"jacobian\":kp_driving_video[0,:,2:].reshape(1,10,2,2)} # kp of the ith frame\n",
    "\n",
    "        # Start generator\n",
    "        for i in range(((x['video'].shape[2]) // frames) * frames):  # cut the last <24 frames\n",
    "            source = x['video'][:, :, 0]\n",
    "            driving = x['video'][:, :, i]\n",
    "            kp_driving = {\"value\": kp_driving_video[i, :, :2], \"jacobian\": kp_driving_video[i, :, 2:]}  # kp of the ith frame\n",
    "            kp_driving['value'] = kp_driving['value'].reshape(1, 10, 2)\n",
    "            kp_driving['jacobian'] = kp_driving['jacobian'].reshape(1, 10, 2, 2)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "            out['kp_source'] = kp_source\n",
    "            out['kp_driving'] = kp_driving\n",
    "            del out['sparse_deformed']\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "            \n",
    "            # Save each frame as PNG to png_dir\n",
    "            frame_name = f\"frame_{i}.png\"\n",
    "            frame_path = os.path.join(png_dir, frame_name)\n",
    "            imageio.imsave(frame_path, (255 * out['prediction'].detach().cpu().numpy()).astype(np.uint8))\n",
    "        i = i + 1\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "09470afd",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dataloader' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#########  FOMM+VRNN ########\u001b[39;00m\n\u001b[1;32m      2\u001b[0m i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it, x \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(\u001b[43mdataloader\u001b[49m)):\n\u001b[1;32m      4\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreconstruction_params\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_videos\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      5\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m it \u001b[38;5;241m>\u001b[39m config[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreconstruction_params\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnum_videos\u001b[39m\u001b[38;5;124m'\u001b[39m]:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dataloader' is not defined"
     ]
    }
   ],
   "source": [
    "#########  FOMM+VRNN ########\n",
    "i = 0\n",
    "for it, x in tqdm(enumerate(dataloader)):\n",
    "        if config['reconstruction_params']['num_videos'] is not None:\n",
    "            if it > config['reconstruction_params']['num_videos']:\n",
    "                break\n",
    "        if i > 0:\n",
    "            break\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            visualizations = []\n",
    "\n",
    "            ######## keypoints ########\n",
    "            kp_driving_video = test_video_unstd_list[it].reshape(-1,10,6)\n",
    "            kp_driving_video = torch.tensor(kp_driving_video)\n",
    "            kp_source = {\"value\":kp_driving_video[0,:,:2].reshape(1,10,2),\"jacobian\":kp_driving_video[0,:,2:].reshape(1,10,2,2)} # kp of the ith frame      \n",
    "        \n",
    "        ##### Start generator\n",
    "        loss_list = []\n",
    "        fvd_list = []\n",
    "        for i in range(((x['video'].shape[2])//frames)*frames): # cut the last <24 frames\n",
    "            source = x['video'][:, :, 0]\n",
    "            driving = x['video'][:, :, i]\n",
    "            kp_driving = {\"value\":kp_driving_video[i,:,:2],\"jacobian\":kp_driving_video[i,:,2:]} # kp of the ith frame\n",
    "            kp_driving['value'] = kp_driving['value'].reshape(1,10,2)\n",
    "            kp_driving['jacobian'] = kp_driving['jacobian'].reshape(1,10,2,2)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "            out['kp_source'] = kp_source\n",
    "            out['kp_driving'] = kp_driving\n",
    "            del out['sparse_deformed']\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "\n",
    "            visualization = Visualizer(**config['visualizer_params']).visualize(source=source,\n",
    "                                                                                    driving=driving, out=out)\n",
    "            visualizations.append(visualization)\n",
    "            # mse loss\n",
    "            if np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean() != 0:\n",
    "                loss_list.append(np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean())\n",
    "                # Calculate FVD for each frame using ground truth and predicted videos\n",
    "                ground_truth_features = driving.detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "                predicted_features = out['prediction'].detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "                fvd_list.append(compute_fvd(ground_truth_features, predicted_features))\n",
    "\n",
    "        print(\"Reconstruction loss: %s\" % np.mean(loss_list))\n",
    "        loss_list_total.append(np.mean(loss_list))\n",
    "\n",
    "        print(\"FVD Score: %s\" % np.mean(fvd_list))\n",
    "        fvd_list_total.append(np.mean(fvd_list))\n",
    "\n",
    "        predictions = np.concatenate(predictions, axis=1)\n",
    "        imageio.imsave(os.path.join(png_dir, x['name'][0] + '.png'), (255 * predictions).astype(np.uint8))\n",
    "        image_name = x['name'][0] + config['reconstruction_params']['format']\n",
    "        imageio.mimsave(os.path.join(log_dir, image_name), visualizations)\n",
    "\n",
    "print(\"mean Reconstruction loss: %s\" % np.mean(loss_list_total)) \n",
    "print(\"mean FVD score: %s\" % np.mean(fvd_list_total)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "763dd68a-13be-4d8b-b49f-fc0f16210996",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
