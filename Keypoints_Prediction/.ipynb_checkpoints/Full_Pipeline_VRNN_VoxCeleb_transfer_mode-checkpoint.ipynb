{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e5a098",
   "metadata": {},
   "source": [
    "# This is a full pipeline with keypoints prediction using VRNN in transfer mode for VoxCeleb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4af7e9",
   "metadata": {},
   "source": [
    "# Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "afa4a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 22:30:18.294287: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-05-20 22:30:19.067133: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "import numpy as np\n",
    "import imageio\n",
    "from Training_Prediction.FOMM.Source_Model.sync_batchnorm import DataParallelWithCallback\n",
    "from Training_Prediction.FOMM.Source_Model.modules.RNN_prediction_module import PredictionModule\n",
    "from Training_Prediction.FOMM.Source_Model.augmentation import SelectRandomFrames, SelectFirstFrames_two, VideoToTensor\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Training_Prediction.FOMM.Source_Model.frames_dataset import FramesDataset_transfer\n",
    "import tensorflow.compat.v1 as tf\n",
    "from Training_Prediction.PREDICTOR.Source_Model.VRNN import build_vrnn, get_config\n",
    "import pickle\n",
    "from Training_Prediction.PREDICTOR.Source_Model.VRNN_prediction import VRNN_predict\n",
    "from Training_Prediction.PREDICTOR.Source_Model.prediction_toplevel import KPDataset,get_data_from_dataloader_60\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "import yaml\n",
    "from Training_Prediction.FOMM.Source_Model.modules.generator import OcclusionAwareGenerator,calculate_frechet_distance,compute_fvd\n",
    "from Training_Prediction.FOMM.Source_Model.modules.keypoint_detector import KPDetector\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer, Visualizer_slow\n",
    "from torch import nn\n",
    "\n",
    "import tensorflow.compat.v1 as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050fe1a-6b18-42f6-b9c5-c797ab8fe3ee",
   "metadata": {},
   "source": [
    "# After VRNN is trained, load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "631fc218-f893-4e25-a74a-15333bfa0666",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 22:30:21.191797: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22296 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:02:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:460: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/distributions/normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:460: kl_divergence (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "model = build_vrnn\n",
    "frames = 24\n",
    "cfg = get_config()\n",
    "input_keypoint = tf.keras.Input(shape=[frames,10,6],name='keypoint')\n",
    "observed_keypoints_stop = tf.keras.layers.Lambda(tf.stop_gradient)(\n",
    "input_keypoint)\n",
    "vrnn_model = model(cfg)\n",
    "predicted_keypoints, kl_divergence = vrnn_model(observed_keypoints_stop)\n",
    "train_model = tf.keras.Model(inputs=[input_keypoint],outputs=[predicted_keypoints])\n",
    "vrnn_coord_pred_loss = tf.nn.l2_loss(\n",
    "observed_keypoints_stop - predicted_keypoints)\n",
    "# Normalize by batch size and sequence length:\n",
    "vrnn_coord_pred_loss /= tf.to_float(\n",
    "  tf.shape(input_keypoint)[0] * tf.shape(input_keypoint)[1])\n",
    "train_model.add_loss(vrnn_coord_pred_loss)\n",
    "kl_loss = tf.reduce_mean(kl_divergence)  # Mean over batch and timesteps.\n",
    "train_model.add_loss(cfg.kl_loss_scale * kl_loss)\n",
    "\n",
    "# Load saved model:\n",
    "cfg = get_config()\n",
    "checkpoint_path = \"Checkpoints/VRNN_3883videos_vox_12-12.ckpt\" #  12-12 frames\n",
    "# checkpoint_path = \"Checkpoints/VRNN_3883videos_vox_6-6.ckpt\" #  6-6 frames\n",
    "\n",
    "# Loads the weights\n",
    "train_model.load_weights(checkpoint_path)\n",
    "train_model.reset_states()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde18681",
   "metadata": {},
   "source": [
    "# Import keypoints of 44 VoxCeleb driving test videos and convert to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a9d3647d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "with open(\"kp_test_44_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "print(len(kp_time_series))\n",
    "\n",
    "#convert list of keypoints to dictionary:\n",
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)\n",
    "    \n",
    "kp_list_test1 = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_test1.append(kp_one_video_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a540f7d",
   "metadata": {},
   "source": [
    "# Import keypoints of 44 VoxCeleb source test videos and convert to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "25aa6e86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n"
     ]
    }
   ],
   "source": [
    "with open(\"kp_test_44_vox_source.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "print(len(kp_time_series))\n",
    "\n",
    "#convert list of keypoints to dictionary:\n",
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)\n",
    "    \n",
    "kp_list_test2 = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_test2.append(kp_one_video_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f9edd0",
   "metadata": {},
   "source": [
    "# Apply min-max std to keypoints of 44 driving test videos and convert to mini-batches: 12 or 24 frames per batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78f49ce2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "(118, 60)\n"
     ]
    }
   ],
   "source": [
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_test_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_test1)):\n",
    "    data = kp_list_test1[video_idx]\n",
    "    data_length = len(kp_list_test1[video_idx])\n",
    "    step_interval = 12 # choose between 12 frames or 24 frames \n",
    "    min_required_steps = 2*step_interval\n",
    "    selected_data = []\n",
    "    for i in range(0, data_length - min_required_steps+1, 2 * step_interval):\n",
    "        selected_data.extend(data[i:i + step_interval])\n",
    "    min_values = np.min(selected_data,axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(selected_data,axis=0) # 60 maxs of one selected video in the loop \n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_test1[video_idx] - min_values) / range_values\n",
    "    kp_list_test_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_test_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bcd21ef0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset batches: 529\n",
      "(24, 60)\n"
     ]
    }
   ],
   "source": [
    "######### convert into batches:\n",
    "frames = min_required_steps\n",
    "input_frames = int(frames / 2)\n",
    "data_batch_test = []\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_test.append(arr)\n",
    "print(f'test dataset batches:', len(data_batch_test))\n",
    "print(data_batch_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e9960c3d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### test dataset:\n",
    "\n",
    "test_data_reshape = np.array(data_batch_test).reshape(-1,frames,60)\n",
    "test_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c19e5-ee36-44cc-a8c7-7caff90e7d29",
   "metadata": {},
   "source": [
    "# Predict keypoints using trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "20c9c688-c031-4a26-8a24-2720d2b21e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 6/17 [=========>....................] - ETA: 0s"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-05-20 22:30:37.808635: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 7s 24ms/step\n",
      "(529, 24, 10, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset and process model.predict():\n",
    "\n",
    "validation_data = test_data_reshape\n",
    "\n",
    "validation_data_tensor = tf.convert_to_tensor(validation_data.reshape(-1,frames,10,6))\n",
    "pred = train_model.predict(validation_data_tensor)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b86c4-b761-48d9-ad9a-88908129633b",
   "metadata": {},
   "source": [
    "# Generate unstd keypoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "27d67c64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches of each video: 44\n"
     ]
    }
   ],
   "source": [
    "# save num_batches for each video:\n",
    "num_batch_video = []\n",
    "num_full_batches_all = 0\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] > frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        num_full_batches_all += num_full_batches\n",
    "        num_batch_video.append(num_full_batches)\n",
    "print(f'number of batches of each video:', len(num_batch_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dacf886e-0203-4086-bedb-4468e1cf4fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first half of frames: groundtruth; last half of frames: predicted\n",
    "test_gt_pred = np.concatenate((test_data_reshape[:,:input_frames], pred.reshape(-1,frames,60)[:,input_frames:]), axis = 1)\n",
    "test_gt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc63c71b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstd for each video:\n",
    "test_video_unstd_list = []\n",
    "for video_idx in range(len(num_batch_video)):\n",
    "    test_video = test_gt_pred[sum(num_batch_video[:video_idx]):sum(num_batch_video[:video_idx+1])]\n",
    "    test_video_unstd = test_video * range_list[video_idx] + min_list[video_idx]\n",
    "    test_video_unstd_list.append(test_video_unstd) # unstd video keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74a397-20c6-4395-b7b9-c14de9621770",
   "metadata": {},
   "source": [
    "# Optical flow and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4cbe6fce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use predefined train-test split.\n",
      "using videos from test directory\n",
      "['id10280#NXjT3732Ekg#001093#001192.mp4', 'id10281#NHARUN9OhSo#000605#000886.mp4', 'id10281#NHARUN9OhSo#001059#001210.mp4', 'id10281#NHARUN9OhSo#002098#002175.mp4', 'id10281#NHARUN9OhSo#002209#002570.mp4', 'id10281#NHARUN9OhSo#006609#006906.mp4', 'id10281#NHARUN9OhSo#006912#007284.mp4', 'id10281#NHARUN9OhSo#007425#007663.mp4', 'id10282#IDA_ElNHLn4#000674#000852.mp4', 'id10282#IDA_ElNHLn4#001226#001390.mp4', 'id10283#N69Hp2DGMLk#000519#000619.mp4', 'id10283#N69Hp2DGMLk#000721#000842.mp4', 'id10283#N69Hp2DGMLk#000893#001589.mp4', 'id10283#N69Hp2DGMLk#004133#005157.mp4', 'id10283#N69Hp2DGMLk#005157#005316.mp4', 'id10283#N69Hp2DGMLk#005931#006184.mp4', 'id10283#N69Hp2DGMLk#006184#006353.mp4', 'id10283#N69Hp2DGMLk#006405#006583.mp4', 'id10283#N69Hp2DGMLk#006600#007118.mp4', 'id10283#N69Hp2DGMLk#007129#007281.mp4', 'id10283#r9-0pljhZqs#002414#002769.mp4', 'id10283#r9-0pljhZqs#003725#003847.mp4', 'id10283#r9-0pljhZqs#004062#004408.mp4', 'id10283#r9-0pljhZqs#007271#007498.mp4', 'id10283#r9-0pljhZqs#007920#008172.mp4', 'id10283#r9-0pljhZqs#008373#008488.mp4', 'id10283#r9-0pljhZqs#009636#009808.mp4', 'id10283#r9-0pljhZqs#010121#010605.mp4', 'id10283#r9-0pljhZqs#010765#010954.mp4', 'id10283#r9-0pljhZqs#011158#011299.mp4', 'id10283#r9-0pljhZqs#012307#012561.mp4', 'id10283#r9-0pljhZqs#013264#013592.mp4', 'id10283#r9-0pljhZqs#013743#013963.mp4', 'id10283#r9-0pljhZqs#014353#014609.mp4', 'id10283#r9-0pljhZqs#014722#014831.mp4', 'id10283#r9-0pljhZqs#014864#015079.mp4', 'id10283#r9-0pljhZqs#015364#015539.mp4', 'id10283#rUnfAxF1dJI#001389#001843.mp4', 'id10283#uA1E_38TuTw#001699#001863.mp4', 'id10283#uA1E_38TuTw#002329#002480.mp4', 'id10283#uA1E_38TuTw#002875#002990.mp4', 'id10283#uA1E_38TuTw#003095#003227.mp4', 'id10283#uA1E_38TuTw#003246#003371.mp4', 'id10283#x6M3KQ8-gM4#000018#000441.mp4']\n",
      "44\n",
      "Use predefined train-test split.\n",
      "using videos from test_source directory\n",
      "['id10283#x6M3KQ8-gM4#005643#006041.mp4', 'id10283#x6M3KQ8-gM4#006204#006291.mp4', 'id10283#x6M3KQ8-gM4#006291#006800.mp4', 'id10284#Rzpm9hlXV6I#005604#005821.mp4', 'id10284#Rzpm9hlXV6I#008487#008631.mp4', 'id10286#9K2YB1d8BqY#000284#000429.mp4', 'id10286#9K2YB1d8BqY#000443#000552.mp4', 'id10286#9K2YB1d8BqY#000781#000963.mp4', 'id10286#9K2YB1d8BqY#000979#001081.mp4', 'id10286#9K2YB1d8BqY#001310#001474.mp4', 'id10286#9K2YB1d8BqY#001493#001925.mp4', 'id10286#9K2YB1d8BqY#002331#002614.mp4', 'id10286#9K2YB1d8BqY#002627#002747.mp4', 'id10286#9K2YB1d8BqY#002759#002883.mp4', 'id10286#9K2YB1d8BqY#002906#003075.mp4', 'id10286#9K2YB1d8BqY#003395#003502.mp4', 'id10286#9K2YB1d8BqY#003518#003707.mp4', 'id10286#9K2YB1d8BqY#003725#003904.mp4', 'id10286#9K2YB1d8BqY#004558#004747.mp4', 'id10286#9K2YB1d8BqY#005197#005375.mp4', 'id10286#9K2YB1d8BqY#005478#005818.mp4', 'id10286#9K2YB1d8BqY#006206#006519.mp4', 'id10286#9K2YB1d8BqY#007056#007176.mp4', 'id10286#9K2YB1d8BqY#007176#007327.mp4', 'id10286#9K2YB1d8BqY#007327#007598.mp4', 'id10287#RfDogGJ-UnM#000900#000988.mp4', 'id10287#RfDogGJ-UnM#000988#001052.mp4', 'id10287#RfDogGJ-UnM#001114#001235.mp4', 'id10289#2dD2ZWGfhwc#002584#002693.mp4', 'id10289#Rn0Z_lIiL1w#000852#001047.mp4', 'id10290#FgJsZ26_AXU#023990#024098.mp4', 'id10290#KacOkTDTi-g#000035#000595.mp4', 'id10290#KacOkTDTi-g#001178#001596.mp4', 'id10291#4aLg_keiGHw#000101#000460.mp4', 'id10291#4aLg_keiGHw#000720#000820.mp4', 'id10291#4aLg_keiGHw#000871#001068.mp4', 'id10291#4aLg_keiGHw#001217#001424.mp4', 'id10291#4aLg_keiGHw#001497#001678.mp4', 'id10291#4aLg_keiGHw#001631#001901.mp4', 'id10291#4aLg_keiGHw#001702#001895.mp4', 'id10291#4aLg_keiGHw#002402#002647.mp4', 'id10291#aQ6uxeakacs#000109#000585.mp4', 'id10291#aQ6uxeakacs#000855#001332.mp4', 'id10291#aQ6uxeakacs#001405#001583.mp4']\n",
      "44\n",
      "Use predefined train-test split.\n",
      "using videos from test_recon directory\n",
      "['id10280#NXjT3732Ekg#001093#001192.mp4.png', 'id10281#NHARUN9OhSo#000605#000886.mp4.png', 'id10281#NHARUN9OhSo#001059#001210.mp4.png', 'id10281#NHARUN9OhSo#002098#002175.mp4.png', 'id10281#NHARUN9OhSo#002209#002570.mp4.png', 'id10281#NHARUN9OhSo#006609#006906.mp4.png', 'id10281#NHARUN9OhSo#006912#007284.mp4.png', 'id10281#NHARUN9OhSo#007425#007663.mp4.png', 'id10282#IDA_ElNHLn4#000674#000852.mp4.png', 'id10282#IDA_ElNHLn4#001226#001390.mp4.png', 'id10283#N69Hp2DGMLk#000519#000619.mp4.png', 'id10283#N69Hp2DGMLk#000721#000842.mp4.png', 'id10283#N69Hp2DGMLk#000893#001589.mp4.png', 'id10283#N69Hp2DGMLk#004133#005157.mp4.png', 'id10283#N69Hp2DGMLk#005157#005316.mp4.png', 'id10283#N69Hp2DGMLk#005931#006184.mp4.png', 'id10283#N69Hp2DGMLk#006184#006353.mp4.png', 'id10283#N69Hp2DGMLk#006405#006583.mp4.png', 'id10283#N69Hp2DGMLk#006600#007118.mp4.png', 'id10283#N69Hp2DGMLk#007129#007281.mp4.png', 'id10283#r9-0pljhZqs#002414#002769.mp4.png', 'id10283#r9-0pljhZqs#003725#003847.mp4.png', 'id10283#r9-0pljhZqs#004062#004408.mp4.png', 'id10283#r9-0pljhZqs#007271#007498.mp4.png', 'id10283#r9-0pljhZqs#007920#008172.mp4.png', 'id10283#r9-0pljhZqs#008373#008488.mp4.png', 'id10283#r9-0pljhZqs#009636#009808.mp4.png', 'id10283#r9-0pljhZqs#010121#010605.mp4.png', 'id10283#r9-0pljhZqs#010765#010954.mp4.png', 'id10283#r9-0pljhZqs#011158#011299.mp4.png', 'id10283#r9-0pljhZqs#012307#012561.mp4.png', 'id10283#r9-0pljhZqs#013264#013592.mp4.png', 'id10283#r9-0pljhZqs#013743#013963.mp4.png', 'id10283#r9-0pljhZqs#014353#014609.mp4.png', 'id10283#r9-0pljhZqs#014722#014831.mp4.png', 'id10283#r9-0pljhZqs#014864#015079.mp4.png', 'id10283#r9-0pljhZqs#015364#015539.mp4.png', 'id10283#rUnfAxF1dJI#001389#001843.mp4.png', 'id10283#uA1E_38TuTw#001699#001863.mp4.png', 'id10283#uA1E_38TuTw#002329#002480.mp4.png', 'id10283#uA1E_38TuTw#002875#002990.mp4.png', 'id10283#uA1E_38TuTw#003095#003227.mp4.png', 'id10283#uA1E_38TuTw#003246#003371.mp4.png', 'id10283#x6M3KQ8-gM4#000018#000441.mp4.png']\n",
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "####### call the config functions and inference dataloader #########\n",
    "\n",
    "config=\"config/abs-vox.yml\"\n",
    "\n",
    "# Test dataset\n",
    "with open(config) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "dataset1 = FramesDataset_transfer(is_train=(False), **config['dataset_params'],mode=\"RNN\") # test: driving videos\n",
    "print(len(dataset1))\n",
    "dataloader1 = DataLoader(dataset1, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "dataset2 = FramesDataset_transfer(is_train=(True), **config['dataset_params'],mode=\"VRNN\") # test_source: source videos\n",
    "print(len(dataset2))\n",
    "dataloader2 = DataLoader(dataset2, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "dataset3 = FramesDataset_transfer(is_train=(False), **config['dataset_params'],mode=\"VRNN\") # test_recon: reference videos(only FOMM, no keypoints prediction)\n",
    "print(len(dataset3))\n",
    "dataloader3 = DataLoader(dataset3, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "### call the functions        \n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                            **config['model_params']['common_params'])\n",
    "\n",
    "log_dir=\"./log/test-reconstruction-vox\"\n",
    "checkpoint=\"./Training_Prediction/FOMM/Trained_Models/vox-cpk.pth.tar\"\n",
    "\n",
    "if checkpoint is not None:\n",
    "    Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
    "else:\n",
    "    raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "png_dir = os.path.join(log_dir, 'prediction/png')\n",
    "log_dir = os.path.join(log_dir, 'prediction')\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(png_dir):\n",
    "    os.makedirs(png_dir)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator = DataParallelWithCallback(generator)\n",
    "    kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "generator.eval()\n",
    "kp_detector.eval()\n",
    "\n",
    "prediction_params = config['prediction_params']\n",
    "\n",
    "num_epochs = prediction_params['num_epochs']\n",
    "lr = prediction_params['lr']\n",
    "bs = prediction_params['batch_size']\n",
    "num_frames = prediction_params['num_frames']\n",
    "loss_list_total = []\n",
    "fvd_list_total = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acf51992",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/opt/conda/lib/python3.11/site-packages/torch/nn/functional.py:4236: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "/home/jovyan/gen-mod-vol/tristen/Motion-Transfer-Keypoints-Prediction/Keypoints_Prediction/Training_Prediction/FOMM/Source_Model/logger.py:111: FutureWarning: `draw.circle` is deprecated in favor of `draw.disk`.`draw.circle` will be removed in version 0.19\n",
      "  rr, cc = circle(kp[1], kp[0], self.kp_size, shape=image.shape[:2])\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.008713429\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1it [01:25, 85.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.005762442\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [07:50, 261.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.0056658736\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [11:10, 233.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.009705139\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [11:58, 160.29s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.0075246124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [22:14, 324.55s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.008632543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [29:10, 355.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.008110646\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [39:19, 438.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.0069862194\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [44:00, 388.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.008677544\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [46:05, 305.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.007177669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [47:55, 245.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.0057533425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [49:45, 204.09s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.0077380887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [1:22:58, 414.91s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 21\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(((x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m/\u001b[39mframes)\u001b[38;5;241m*\u001b[39mframes): \u001b[38;5;66;03m# cut the last <24 frames\u001b[39;00m\n\u001b[1;32m     20\u001b[0m     driving \u001b[38;5;241m=\u001b[39m x[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m][:, :, i]\n\u001b[0;32m---> 21\u001b[0m     source \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[43mdataset2\u001b[49m\u001b[43m[\u001b[49m\u001b[43mit\u001b[49m\u001b[43m]\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m][:,\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m) \u001b[38;5;66;03m# source frame from set2 \u001b[39;00m\n\u001b[1;32m     22\u001b[0m     driving_reference \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(dataset3[it][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvideo\u001b[39m\u001b[38;5;124m'\u001b[39m][:,i])\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m256\u001b[39m,\u001b[38;5;241m256\u001b[39m)\n\u001b[1;32m     23\u001b[0m     kp_driving \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m:kp_driving_video[i,:,:\u001b[38;5;241m2\u001b[39m],\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m:kp_driving_video[i,:,\u001b[38;5;241m2\u001b[39m:]} \u001b[38;5;66;03m# kp of the ith frame\u001b[39;00m\n",
      "File \u001b[0;32m~/gen-mod-vol/tristen/Motion-Transfer-Keypoints-Prediction/Keypoints_Prediction/Training_Prediction/FOMM/Source_Model/frames_dataset.py:228\u001b[0m, in \u001b[0;36mFramesDataset_transfer.__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m    226\u001b[0m     video_array \u001b[38;5;241m=\u001b[39m [img_as_float32(io\u001b[38;5;241m.\u001b[39mimread(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(path, frames[idx]))) \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m frame_idx]\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 228\u001b[0m     video_array \u001b[38;5;241m=\u001b[39m \u001b[43mread_video\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_shape\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mframe_shape\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    229\u001b[0m     num_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(video_array)\n\u001b[1;32m    230\u001b[0m     frame_idx \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msort(np\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mchoice(num_frames, replace\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_train \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\n\u001b[1;32m    231\u001b[0m         num_frames)\n",
      "File \u001b[0;32m~/gen-mod-vol/tristen/Motion-Transfer-Keypoints-Prediction/Keypoints_Prediction/Training_Prediction/FOMM/Source_Model/frames_dataset.py:43\u001b[0m, in \u001b[0;36mread_video\u001b[0;34m(name, frame_shape)\u001b[0m\n\u001b[1;32m     41\u001b[0m     video_array \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmoveaxis(video_array, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.gif\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m name\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mp4\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m name\u001b[38;5;241m.\u001b[39mlower()\u001b[38;5;241m.\u001b[39mendswith(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.mov\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[0;32m---> 43\u001b[0m     video \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\u001b[43mmimread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(video\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m     45\u001b[0m         video \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray([gray2rgb(frame) \u001b[38;5;28;01mfor\u001b[39;00m frame \u001b[38;5;129;01min\u001b[39;00m video])\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/imageio/v2.py:451\u001b[0m, in \u001b[0;36mmimread\u001b[0;34m(uri, format, memtest, **kwargs)\u001b[0m\n\u001b[1;32m    449\u001b[0m imopen_args[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlegacy_mode\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    450\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m imopen(uri, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrI\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mimopen_args) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m--> 451\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m file\u001b[38;5;241m.\u001b[39miter(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m    452\u001b[0m         images\u001b[38;5;241m.\u001b[39mappend(image)\n\u001b[1;32m    453\u001b[0m         nbytes \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m image\u001b[38;5;241m.\u001b[39mnbytes\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/imageio/core/legacy_plugin_wrapper.py:269\u001b[0m, in \u001b[0;36mLegacyPlugin.iter\u001b[0;34m(self, **kwargs)\u001b[0m\n\u001b[1;32m    258\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Iterate over a list of ndimages given by the URI\u001b[39;00m\n\u001b[1;32m    259\u001b[0m \n\u001b[1;32m    260\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[38;5;124;03m    format.\u001b[39;00m\n\u001b[1;32m    266\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    268\u001b[0m reader \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlegacy_get_reader(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m--> 269\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m image \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m    270\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m image\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/imageio/core/format.py:501\u001b[0m, in \u001b[0;36mFormat.Reader.iter_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    499\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[1;32m    500\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m--> 501\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[43mArray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmeta\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    502\u001b[0m i \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/imageio/core/util.py:129\u001b[0m, in \u001b[0;36mArray.__new__\u001b[0;34m(cls, array, meta)\u001b[0m\n\u001b[1;32m    127\u001b[0m meta \u001b[38;5;241m=\u001b[39m meta \u001b[38;5;28;01mif\u001b[39;00m meta \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(array, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmeta\u001b[39m\u001b[38;5;124m\"\u001b[39m, {})\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 129\u001b[0m     ob \u001b[38;5;241m=\u001b[39m array\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;28mcls\u001b[39m)\n\u001b[1;32m    130\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m:  \u001b[38;5;66;03m# pragma: no cover\u001b[39;00m\n\u001b[1;32m    131\u001b[0m     \u001b[38;5;66;03m# Just return the original; no metadata on the array in Pypy!\u001b[39;00m\n\u001b[1;32m    132\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m array\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/imageio/core/util.py:156\u001b[0m, in \u001b[0;36mArray.__array_finalize__\u001b[0;34m(self, ob)\u001b[0m\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copy_meta(ob\u001b[38;5;241m.\u001b[39mmeta)\n\u001b[1;32m    155\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 156\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_copy_meta\u001b[49m\u001b[43m(\u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.11/site-packages/imageio/core/util.py:138\u001b[0m, in \u001b[0;36mArray._copy_meta\u001b[0;34m(self, meta)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_copy_meta\u001b[39m(\u001b[38;5;28mself\u001b[39m, meta):\n\u001b[1;32m    137\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Make a 2-level deep copy of the meta dictionary.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_meta \u001b[38;5;241m=\u001b[39m \u001b[43mDict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    139\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m meta\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    140\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val, \u001b[38;5;28mdict\u001b[39m):\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#########  FOMM+VRNN ########\n",
    "\n",
    "for it, x in tqdm(enumerate(dataloader1)):\n",
    "        if config['reconstruction_params']['num_videos'] is not None:\n",
    "            if it > config['reconstruction_params']['num_videos']:\n",
    "                break\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            visualizations = []\n",
    "\n",
    "            ######## keypoints ########\n",
    "            kp_driving_video = test_video_unstd_list[it].reshape(-1,10,6) \n",
    "            kp_driving_video = torch.tensor(kp_driving_video)\n",
    "            kp_source = {\"value\":torch.tensor(kp_list_test2[it][0]).reshape(10,6)[:,:2],\"jacobian\":torch.tensor(kp_list_test2[it][0]).reshape(10,6)[:,2:].reshape(1,10,2,2)} # kp of the ith frame      \n",
    "        \n",
    "        ##### Start generator\n",
    "        loss_list = []\n",
    "        fvd_list = []\n",
    "        for i in range(((x['video'].shape[2])//frames)*frames): # cut the last <24 frames\n",
    "            driving = x['video'][:, :, i]\n",
    "            source = torch.tensor(dataset2[it]['video'][:,0]).reshape(1,3,256,256) # source frame from set2 \n",
    "            driving_reference = torch.tensor(dataset3[it]['video'][:,i]).reshape(1,3,256,256)\n",
    "            kp_driving = {\"value\":kp_driving_video[i,:,:2],\"jacobian\":kp_driving_video[i,:,2:]} # kp of the ith frame\n",
    "            kp_driving['value'] = kp_driving['value'].reshape(1,10,2)\n",
    "            kp_driving['jacobian'] = kp_driving['jacobian'].reshape(1,10,2,2)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "            out['kp_source'] = kp_source\n",
    "            out['kp_driving'] = kp_driving\n",
    "            del out['sparse_deformed']\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "\n",
    "            visualization = Visualizer(**config['visualizer_params']).visualize(source=source,\n",
    "                                                                                    driving=driving, out=out)\n",
    "            visualizations.append(visualization) #visualizations[0].shape: (256, 1280, 3)\n",
    "            # mse loss\n",
    "            if np.abs(out['prediction'].detach().cpu().numpy() - driving_reference.cpu().numpy()).mean() != 0:\n",
    "                loss_list.append(np.abs(out['prediction'].detach().cpu().numpy() - driving_reference.cpu().numpy()).mean())\n",
    "                # Calculate FVD for each frame using ground truth and predicted videos\n",
    "#                ground_truth_features = driving_reference.detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "#                predicted_features = out['prediction'].detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "#                fvd_list.append(compute_fvd(ground_truth_features, predicted_features))\n",
    "\n",
    "        print(\"Reconstruction loss: %s\" % np.mean(loss_list))\n",
    "        loss_list_total.append(np.mean(loss_list))\n",
    "        \n",
    "#        print(\"FVD Score: %s\" % np.mean(fvd_list))\n",
    "#        fvd_list_total.append(np.mean(fvd_list))\n",
    "\n",
    "        predictions = np.concatenate(predictions, axis=1)\n",
    "        imageio.imsave(os.path.join(png_dir, x['name'][0] + '.png'), (255 * predictions).astype(np.uint8))\n",
    "        image_name = x['name'][0] + config['reconstruction_params']['format']\n",
    "        imageio.mimsave(os.path.join(log_dir, image_name), visualizations)\n",
    "\n",
    "print(\"mean Reconstruction loss: %s\" % np.mean(loss_list_total)) \n",
    "#print(\"mean FVD score: %s\" % np.mean(fvd_list_total))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
