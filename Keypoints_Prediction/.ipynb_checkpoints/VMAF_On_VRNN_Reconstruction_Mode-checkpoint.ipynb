{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0ba9bcc3-d7fb-42a3-9b66-68f095bd0876",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 14:40:34.361849: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 14:40:34.974173: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "import numpy as np\n",
    "import imageio\n",
    "from Training_Prediction.FOMM.Source_Model.sync_batchnorm import DataParallelWithCallback\n",
    "from Training_Prediction.FOMM.Source_Model.modules.RNN_prediction_module import PredictionModule\n",
    "from Training_Prediction.FOMM.Source_Model.augmentation import SelectRandomFrames, SelectFirstFrames_two, VideoToTensor\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Training_Prediction.FOMM.Source_Model.frames_dataset import FramesDataset\n",
    "import tensorflow.compat.v1 as tf\n",
    "from Training_Prediction.PREDICTOR.Source_Model.VRNN import build_vrnn, get_config\n",
    "import pickle\n",
    "from Training_Prediction.PREDICTOR.Source_Model.VRNN_prediction import VRNN_predict\n",
    "from Training_Prediction.PREDICTOR.Source_Model.prediction_toplevel import KPDataset,get_data_from_dataloader_60\n",
    "\n",
    "import gc\n",
    "import pickle\n",
    "import yaml\n",
    "from Training_Prediction.FOMM.Source_Model.modules.generator import OcclusionAwareGenerator,calculate_frechet_distance,compute_fvd\n",
    "from Training_Prediction.FOMM.Source_Model.modules.keypoint_detector import KPDetector\n",
    "#from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer, Visualizer_slow\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "from torch import nn\n",
    "import shutil\n",
    "import vmaf\n",
    "import subprocess\n",
    "import imageio\n",
    "import tensorflow.compat.v1 as tf\n",
    "import json\n",
    "import os, sys\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "582128c0-eae3-4b69-ba65-b241b580a39d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_frames_as_png(frames, png_dir):\n",
    "    for idx, frame in enumerate(frames):\n",
    "        frame_path = os.path.join(png_dir, f'video_frame_{idx:04d}.png')\n",
    "        imageio.imsave(frame_path, (255 * frame).astype(np.uint8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7557659a-6acf-478a-8e72-bccc3f6661ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_png_to_y4m(png_dir, y4m_dir, video_number):\n",
    "    if not png_dir.endswith('/'):\n",
    "        png_dir += '/'\n",
    "    if not y4m_dir.endswith('/'):\n",
    "        y4m_dir += '/'\n",
    "    \n",
    "    input_pattern = f\"{png_dir}video_frame_%04d.png\"\n",
    "    output_file = f\"{y4m_dir}video_{video_number:02d}.y4m\"\n",
    "    \n",
    "    command = (f'ffmpeg -y -i {input_pattern} -r 24 -pix_fmt yuv444p {output_file}')    \n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e9f8f362-574b-4270-b398-c8e50f38db74",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_vmaf(driving_vid, generated_vid, output_path):\n",
    "    if driving_vid[-3:] != 'y4m':\n",
    "         raise Exception('Video must be in y4m format.')\n",
    "    if generated_vid[-3:] != 'y4m':\n",
    "         raise Exception('Video must be in y4m format.')\n",
    "    if output_path[-4:] != 'json':\n",
    "        raise Exception('Output file must be in json format')\n",
    "    \n",
    "    command = (f'vmaf --reference {driving_vid} --distorted {generated_vid} --model version=vmaf_v0.6.1 --output {output_path} --json')\n",
    "    os.system(command)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f0cb0e6-3eff-4b9e-8d3f-2c27cb5b5521",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After VRNN is trained, load the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03201f23-2121-4e33-b4d5-dd1dfae6e18c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:460: Normal.__init__ (from tensorflow.python.ops.distributions.normal) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/ops/distributions/normal.py:149: Distribution.__init__ (from tensorflow.python.ops.distributions.distribution) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n",
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/autograph/impl/api.py:460: kl_divergence (from tensorflow.python.ops.distributions.kullback_leibler) is deprecated and will be removed after 2019-01-01.\n",
      "Instructions for updating:\n",
      "The TensorFlow Distributions library has moved to TensorFlow Probability (https://github.com/tensorflow/probability). You should update all references to use `tfp.distributions` instead of `tf.distributions`.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 14:41:05.317949: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22296 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:e1:00.0, compute capability: 8.6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.11/site-packages/tensorflow/python/util/dispatch.py:1176: to_float (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use `tf.cast` instead.\n"
     ]
    }
   ],
   "source": [
    "model = build_vrnn\n",
    "frames = 24\n",
    "cfg = get_config()\n",
    "input_keypoint = tf.keras.Input(shape=[frames,10,6],name='keypoint')\n",
    "observed_keypoints_stop = tf.keras.layers.Lambda(tf.stop_gradient)(\n",
    "input_keypoint)\n",
    "vrnn_model = model(cfg)\n",
    "predicted_keypoints, kl_divergence = vrnn_model(observed_keypoints_stop)\n",
    "train_model = tf.keras.Model(inputs=[input_keypoint],outputs=[predicted_keypoints])\n",
    "vrnn_coord_pred_loss = tf.nn.l2_loss(\n",
    "observed_keypoints_stop - predicted_keypoints)\n",
    "# Normalize by batch size and sequence length:\n",
    "vrnn_coord_pred_loss /= tf.to_float(tf.shape(input_keypoint)[0] * tf.shape(input_keypoint)[1])\n",
    "train_model.add_loss(vrnn_coord_pred_loss)\n",
    "kl_loss = tf.reduce_mean(kl_divergence)  # Mean over batch and timesteps.\n",
    "train_model.add_loss(cfg.kl_loss_scale * kl_loss)\n",
    "\n",
    "# Load saved model:\n",
    "cfg = get_config()\n",
    "checkpoint_path = \"Checkpoints/VRNN_3883videos_vox_12-12.ckpt\" \n",
    "# checkpoint_path = \"Checkpoints/VRNN_3883videos_vox_6-6.ckpt\" \n",
    "\n",
    "# Loads the weights\n",
    "train_model.load_weights(checkpoint_path)\n",
    "train_model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "760186e6-6d66-42da-8660-dc6c3e9337d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import keypoints of 44 VoxCeleb test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "67848697-e3d4-420b-b994-bb80509d8d0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kp_test_44_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "\n",
    "#### SKIP EVERYTHING UNTIL test_video_unstd_list TO DO WITHOUT PREDICTION ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ae235456-fcb2-4d4f-babd-6aa1970cc6d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convert list of keypoints to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e44ce26b-9142-4c96-8e06-b025240c967e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cf718625-78b0-4998-a6a7-fb8f78a4303a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Apply min-max std to keypoints and convert to batchesÂ¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c749f225-0d5d-49ed-a772-211d2f99ee8d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "(118, 60)\n"
     ]
    }
   ],
   "source": [
    "kp_list_test = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_test.append(kp_one_video_array)\n",
    "    \n",
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_test_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_test)):\n",
    "    data = kp_list_test[video_idx]\n",
    "    data_length = len(kp_list_test[video_idx])\n",
    "    step_interval = 12 # choose between 12 frames or 24 frames \n",
    "    min_required_steps = 2*step_interval\n",
    "    selected_data = []\n",
    "    for i in range(0, data_length - min_required_steps+1, 2 * step_interval):\n",
    "        selected_data.extend(data[i:i + step_interval])\n",
    "    min_values = np.min(selected_data,axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(selected_data,axis=0) # 60 maxs of one selected video in the loop \n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_test[video_idx] - min_values) / range_values\n",
    "    kp_list_test_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_test_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cff27bfc-1604-4de9-a6bd-5ee4d12900f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset batches: 529\n",
      "(24, 60)\n"
     ]
    }
   ],
   "source": [
    "######### convert into batches:\n",
    "frames = min_required_steps\n",
    "input_frames = int(frames / 2)\n",
    "data_batch_test = []\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_test.append(arr)\n",
    "print(f'test dataset batches:', len(data_batch_test))\n",
    "print(data_batch_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a3758ebf-1d93-47dd-bf71-006d87504eba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### test dataset:\n",
    "\n",
    "test_data_reshape = np.array(data_batch_test).reshape(-1,frames,60)\n",
    "test_data_reshape.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f1214da7-1de9-45fa-bffc-035bd897f0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Predict keypoints using trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a896c38c-29f6-498e-9ee0-83c1d00d92f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 4/17 [======>.......................] - ETA: 0s  "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 14:41:47.974218: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17/17 [==============================] - 5s 17ms/step\n",
      "(529, 24, 10, 6)\n"
     ]
    }
   ],
   "source": [
    "# Load test dataset and process model.predict():\n",
    "\n",
    "validation_data = test_data_reshape\n",
    "\n",
    "validation_data_tensor = tf.convert_to_tensor(validation_data.reshape(-1,frames,10,6))\n",
    "pred = train_model.predict(validation_data_tensor)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0914d16b-a55d-4bd0-aad2-8a1df696e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Generate unstd keypoints:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "df5692b5-bd49-4e0c-85a1-0c7509f58fdf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save num_batches for each video:\n",
    "num_batch_video = []\n",
    "num_full_batches_all = 0\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        num_full_batches_all += num_full_batches\n",
    "        num_batch_video.append(num_full_batches)\n",
    "#print(f'number of batches of each video:', len(num_batch_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "43c327b5-9ea8-487a-93f0-1231e74ecf31",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first half of frames: groundtruth; last half of frames: predicted\n",
    "test_gt_pred = np.concatenate((test_data_reshape[:,:input_frames], pred.reshape(-1,frames,60)[:,input_frames:]), axis = 1)\n",
    "test_gt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "12d59edd-69e2-4947-9f74-4e0f08c5015a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### FOR DOING THIS WITHOUT PREDICTION, RUN THE FOLLOWING ####\n",
    "test_video_unstd_list = kp_time_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c5234e29-cb45-414e-a765-8153fb811171",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstd for each video: #### WITH PREDICTION ####\n",
    "test_video_unstd_list = []\n",
    "for video_idx in range(len(num_batch_video)):\n",
    "    test_video = test_gt_pred[sum(num_batch_video[:video_idx]):sum(num_batch_video[:video_idx+1])]\n",
    "    test_video_unstd = test_video * range_list[video_idx] + min_list[video_idx]\n",
    "    test_video_unstd_list.append(test_video_unstd) # unstd video keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b999ce7f-792e-429f-bc55-46f57eb28b0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use predefined train-test split.\n",
      "using videos from test directory\n",
      "['id10280#NXjT3732Ekg#001093#001192.mp4', 'id10281#NHARUN9OhSo#000605#000886.mp4', 'id10281#NHARUN9OhSo#001059#001210.mp4', 'id10281#NHARUN9OhSo#002098#002175.mp4', 'id10281#NHARUN9OhSo#002209#002570.mp4', 'id10281#NHARUN9OhSo#006609#006906.mp4', 'id10281#NHARUN9OhSo#006912#007284.mp4', 'id10281#NHARUN9OhSo#007425#007663.mp4', 'id10282#IDA_ElNHLn4#000674#000852.mp4', 'id10282#IDA_ElNHLn4#001226#001390.mp4', 'id10283#N69Hp2DGMLk#000519#000619.mp4', 'id10283#N69Hp2DGMLk#000721#000842.mp4', 'id10283#N69Hp2DGMLk#000893#001589.mp4', 'id10283#N69Hp2DGMLk#004133#005157.mp4', 'id10283#N69Hp2DGMLk#005157#005316.mp4', 'id10283#N69Hp2DGMLk#005931#006184.mp4', 'id10283#N69Hp2DGMLk#006184#006353.mp4', 'id10283#N69Hp2DGMLk#006405#006583.mp4', 'id10283#N69Hp2DGMLk#006600#007118.mp4', 'id10283#N69Hp2DGMLk#007129#007281.mp4', 'id10283#r9-0pljhZqs#002414#002769.mp4', 'id10283#r9-0pljhZqs#003725#003847.mp4', 'id10283#r9-0pljhZqs#004062#004408.mp4', 'id10283#r9-0pljhZqs#007271#007498.mp4', 'id10283#r9-0pljhZqs#007920#008172.mp4', 'id10283#r9-0pljhZqs#008373#008488.mp4', 'id10283#r9-0pljhZqs#009636#009808.mp4', 'id10283#r9-0pljhZqs#010121#010605.mp4', 'id10283#r9-0pljhZqs#010765#010954.mp4', 'id10283#r9-0pljhZqs#011158#011299.mp4', 'id10283#r9-0pljhZqs#012307#012561.mp4', 'id10283#r9-0pljhZqs#013264#013592.mp4', 'id10283#r9-0pljhZqs#013743#013963.mp4', 'id10283#r9-0pljhZqs#014353#014609.mp4', 'id10283#r9-0pljhZqs#014722#014831.mp4', 'id10283#r9-0pljhZqs#014864#015079.mp4', 'id10283#r9-0pljhZqs#015364#015539.mp4', 'id10283#rUnfAxF1dJI#001389#001843.mp4', 'id10283#uA1E_38TuTw#001699#001863.mp4', 'id10283#uA1E_38TuTw#002329#002480.mp4', 'id10283#uA1E_38TuTw#002875#002990.mp4', 'id10283#uA1E_38TuTw#003095#003227.mp4', 'id10283#uA1E_38TuTw#003246#003371.mp4', 'id10283#x6M3KQ8-gM4#000018#000441.mp4']\n",
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.11/site-packages/torch/functional.py:504: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3483.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "####### call the config functions and inference dataloader #########\n",
    "config=\"config/abs-vox.yml\"\n",
    "\n",
    "# Test dataset\n",
    "with open(config) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "dataset = FramesDataset(is_train=(False), **config['dataset_params'],mode=\"RNN\") # test\n",
    "\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "### call the functions        \n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                            **config['model_params']['common_params'])\n",
    "\n",
    "log_dir=\"./log/test-reconstruction-vox\"\n",
    "checkpoint=\"./Training_Prediction/FOMM/Trained_Models/vox-cpk.pth.tar\"\n",
    "\n",
    "if checkpoint is not None:\n",
    "    Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
    "else:\n",
    "    raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "def save_obj(obj, name):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "pred_png_dir = os.path.join(log_dir, 'prediction/prediction_png')\n",
    "pred_mp4_dir = os.path.join(log_dir, 'prediction/prediction_mp4')\n",
    "pred_y4m_dir = os.path.join(log_dir, 'prediction/prediction_y4m')\n",
    "driving_png_dir = os.path.join(log_dir, 'prediction/driving_png')\n",
    "driving_mp4_dir = os.path.join(log_dir, 'prediction/driving_mp4')\n",
    "driving_y4m_dir = os.path.join(log_dir, 'prediction/driving_y4m')\n",
    "vrnn_vmaf_output = os.path.join(log_dir, 'prediction/vmaf_json_output_vrnn')\n",
    "\n",
    "\n",
    "log_dir = os.path.join(log_dir, 'prediction')\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(pred_png_dir):\n",
    "    os.makedirs(pred_png_dir)\n",
    "if not os.path.exists(pred_mp4_dir):\n",
    "    os.makedirs(pred_mp4_dir)\n",
    "if not os.path.exists(pred_y4m_dir):\n",
    "    os.makedirs(pred_y4m_dir)\n",
    "\n",
    "if not os.path.exists(driving_png_dir):\n",
    "    os.makedirs(driving_png_dir)\n",
    "if not os.path.exists(driving_mp4_dir):\n",
    "    os.makedirs(driving_mp4_dir)\n",
    "if not os.path.exists(driving_y4m_dir):\n",
    "    os.makedirs(driving_y4m_dir)\n",
    "\n",
    "if not os.path.exists(vrnn_vmaf_output):\n",
    "    os.makedirs(vrnn_vmaf_output)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator = DataParallelWithCallback(generator)\n",
    "    kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "generator.eval()\n",
    "kp_detector.eval()\n",
    "\n",
    "prediction_params = config['prediction_params']\n",
    "\n",
    "num_epochs = prediction_params['num_epochs']\n",
    "lr = prediction_params['lr']\n",
    "bs = prediction_params['batch_size']\n",
    "num_frames = prediction_params['num_frames']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "850c2783-9b3b-4f7b-9f7f-641cde71c6ae",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'dict' object has no attribute 'reshape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 19\u001b[0m\n\u001b[1;32m     16\u001b[0m driving_frames \u001b[38;5;241m=\u001b[39m[]\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m######## keypoints ########\u001b[39;00m\n\u001b[0;32m---> 19\u001b[0m kp_driving_video \u001b[38;5;241m=\u001b[39m \u001b[43mtest_video_unstd_list\u001b[49m\u001b[43m[\u001b[49m\u001b[43mit\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreshape\u001b[49m(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m)\n\u001b[1;32m     20\u001b[0m kp_driving_video \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor(kp_driving_video)\n\u001b[1;32m     21\u001b[0m kp_source \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mvalue\u001b[39m\u001b[38;5;124m\"\u001b[39m: kp_driving_video[\u001b[38;5;241m0\u001b[39m, :, :\u001b[38;5;241m2\u001b[39m]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m2\u001b[39m),\n\u001b[1;32m     22\u001b[0m              \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mjacobian\u001b[39m\u001b[38;5;124m\"\u001b[39m: kp_driving_video[\u001b[38;5;241m0\u001b[39m, :, \u001b[38;5;241m2\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m)}  \u001b[38;5;66;03m# kp of the ith frame\u001b[39;00m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'dict' object has no attribute 'reshape'"
     ]
    }
   ],
   "source": [
    "mse_list_videos = []\n",
    "for it, x in tqdm(enumerate(dataloader)):\n",
    "    if config['reconstruction_params']['num_videos'] is not None:\n",
    "        if it > config['reconstruction_params']['num_videos']:\n",
    "            break\n",
    "    \n",
    "    # Clear the PNG directories for each iteration of the outer loop\n",
    "    shutil.rmtree(pred_png_dir, ignore_errors=True)\n",
    "    os.makedirs(pred_png_dir)\n",
    "    \n",
    "    shutil.rmtree(driving_png_dir, ignore_errors=True)\n",
    "    os.makedirs(driving_png_dir)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = []\n",
    "        driving_frames =[]\n",
    "        \n",
    "        ######## keypoints ########\n",
    "        kp_driving_video = test_video_unstd_list[it].reshape(-1, 10, 6)\n",
    "        kp_driving_video = torch.tensor(kp_driving_video)\n",
    "        kp_source = {\"value\": kp_driving_video[0, :, :2].reshape(1, 10, 2),\n",
    "                     \"jacobian\": kp_driving_video[0, :, 2:].reshape(1, 10, 2, 2)}  # kp of the ith frame\n",
    "    \n",
    "    mse_list_frames = []\n",
    "    # Start generator\n",
    "    for i in range(((x['video'].shape[2]) // frames) * frames):  # cut the last <24 frames\n",
    "        source = x['video'][:, :, 0]\n",
    "        driving = x['video'][:, :, i]\n",
    "        kp_driving = {\"value\": kp_driving_video[i, :, :2], \"jacobian\": kp_driving_video[i, :, 2:]}  # kp of the ith frame\n",
    "        kp_driving['value'] = kp_driving['value'].reshape(1, 10, 2)\n",
    "        kp_driving['jacobian'] = kp_driving['jacobian'].reshape(1, 10, 2, 2)\n",
    "        out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "        out['kp_source'] = kp_source\n",
    "        out['kp_driving'] = kp_driving\n",
    "        del out['sparse_deformed']\n",
    "        driving_frames.append(np.transpose(driving.data.cpu().numpy(),[0, 2, 3, 1])[0])\n",
    "        predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "        mse_list_frames.append(np.mean(np.square(driving.cpu().numpy() - out['prediction'].detach().cpu().numpy())))\n",
    "    mse_list_videos.append(np.mean(mse_list_frames))\n",
    "\n",
    "#    predictions = np.concatenate(predictions, axis=1)\n",
    "    \n",
    "    # Save predictions as PNG files\n",
    "#    save_frames_as_png(predictions, pred_png_dir)\n",
    "#    save_frames_as_png(driving_frames,driving_png_dir)\n",
    "\n",
    "    #Convert png files to a y4m format video\n",
    "#    convert_png_to_y4m(pred_png_dir, pred_y4m_dir, it)\n",
    "#    convert_png_to_y4m(driving_png_dir, driving_y4m_dir, it)\n",
    "\n",
    "    #Run VMAF on the videos\n",
    "#    run_vmaf(driving_y4m_dir+f'/video_{it:02d}.y4m',pred_y4m_dir+f'/video_{it:02d}.y4m',vrnn_vmaf_output+f'/vmaf_{it:02d}.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f680547-cffe-46ed-b4bf-11d0c3c08445",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
