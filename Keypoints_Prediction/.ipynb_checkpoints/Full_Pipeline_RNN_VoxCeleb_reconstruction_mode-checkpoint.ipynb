{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "09e5a098",
   "metadata": {},
   "source": [
    "# This is a full pipeline with keypoints prediction using RNN in reconstruction mode for VoxCeleb dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef4af7e9",
   "metadata": {},
   "source": [
    "# Import functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afa4a0f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-18 01:38:01.324445: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2024-07-18 01:38:01.336312: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2024-07-18 01:38:01.339780: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-07-18 01:38:01.351297: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-07-18 01:38:02.062705: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os   \n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "import numpy as np\n",
    "import imageio\n",
    "from Training_Prediction.FOMM.Source_Model.sync_batchnorm import DataParallelWithCallback\n",
    "from Training_Prediction.PREDICTOR.RNN import GRUModel\n",
    "from Training_Prediction.FOMM.Source_Model.augmentation import SelectRandomFrames, SelectFirstFrames_two, VideoToTensor\n",
    "from tqdm import trange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from Training_Prediction.FOMM.Source_Model.frames_dataset import FramesDataset\n",
    "import tensorflow.compat.v1 as tf\n",
    "import pickle\n",
    "from Training_Prediction.PREDICTOR.Source_Model.prediction_toplevel import KPDataset,get_data_from_dataloader_60\n",
    "import gc\n",
    "import pickle\n",
    "import yaml\n",
    "from Training_Prediction.FOMM.Source_Model.modules.generator import OcclusionAwareGenerator,calculate_frechet_distance,compute_fvd\n",
    "from Training_Prediction.FOMM.Source_Model.modules.keypoint_detector import KPDetector\n",
    "#from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer, Visualizer_slow\n",
    "from Training_Prediction.FOMM.Source_Model.logger import Logger, Visualizer\n",
    "from torch import nn\n",
    "import tensorflow.compat.v1 as tf\n",
    "from torch.autograd import Variable\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "import os, sys\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]='0'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47951536",
   "metadata": {},
   "source": [
    "# Import keypoints of 3883 training videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aaa8d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"kp_train_3883_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "len(kp_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "881775e8",
   "metadata": {},
   "source": [
    "# Convert list of keypoints to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49eaf979",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28f2996e",
   "metadata": {},
   "source": [
    "# Apply min-max standardization to keypoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75e9981e",
   "metadata": {},
   "outputs": [],
   "source": [
    "kp_list_train = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_train.append(kp_one_video_array)\n",
    "    \n",
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_train_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_train)):\n",
    "    min_values = np.min(kp_list_train[video_idx],axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(kp_list_train[video_idx],axis=0) # 60 maxs of one selected video in the loop\n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_train[video_idx] - min_values) / range_values\n",
    "    kp_list_train_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_train_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbac9895",
   "metadata": {},
   "source": [
    "# Convert standardized keypoints to mini-batches: 12 or 24 frames a batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e41ef71",
   "metadata": {},
   "outputs": [],
   "source": [
    "######### convert data into batches #########\n",
    "data_batch_train = []\n",
    "\n",
    "frames = 24 # 24 as one batch, use 12 ground truth frames as input to predict next 12 frames as output\n",
    "input_frames = int(frames / 2) \n",
    "input_dim = 60\n",
    "for t,x in enumerate(kp_list_train_std):\n",
    "    if x.shape[0] > frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_train.append(arr)\n",
    "print(f'train dataset batches:', len(data_batch_train))\n",
    "print(data_batch_train[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bce9fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "##### train dataset:\n",
    "\n",
    "train_data_reshape = np.array(data_batch_train).reshape(-1,frames,60)\n",
    "train_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d5e6305",
   "metadata": {},
   "source": [
    "# Define RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0265767c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the model \n",
    "input_dim = 60\n",
    "hidden_dim = 256\n",
    "output_dim = input_dim\n",
    "num_layers = 3\n",
    "learning_rate = 0.001\n",
    "num_epochs = 100\n",
    "model = GRUModel(input_dim, hidden_dim, output_dim, num_layers)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab338ad",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0aa55421",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_data_reshape' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model \u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_data \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_data_reshape\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Convert train_data to DataLoader for efficient batching and shuffling\u001b[39;00m\n\u001b[1;32m      5\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m250\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_data_reshape' is not defined"
     ]
    }
   ],
   "source": [
    "# Train the model \n",
    "train_data = train_data_reshape\n",
    "\n",
    "# Convert train_data to DataLoader for efficient batching and shuffling\n",
    "train_loader = DataLoader(train_data, batch_size=250, shuffle=True)\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    for step, batch in enumerate(train_loader):\n",
    "\n",
    "        batch_input = batch[:,:input_frames]\n",
    "        batch_output = batch[:,input_frames:]\n",
    "        outputs = model(batch_input)\n",
    "\n",
    "        loss = criterion(outputs, batch_output)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs}, Step {step + 1}/{len(train_loader)}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "# Save the best model's state_dict to a file\n",
    "torch.save(model.state_dict(), 'Checkpoints/RNN_3883videos_vox_12-12.pth') # 12-12 frames\n",
    "# torch.save(model.state_dict(), 'Checkpoints/RNN_3883videos_vox_6-6.pth') # 6-6 frames"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8f25b2a",
   "metadata": {},
   "source": [
    "# After RNN is trained, load saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f8454dba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GRUModel(\n",
       "  (gru): GRU(60, 256, num_layers=3, batch_first=True)\n",
       "  (fc): Linear(in_features=256, out_features=60, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the saved parameters\n",
    "model.load_state_dict(torch.load('Checkpoints/RNN_3883videos_vox_12-12.pth')) # 12-12 frames\n",
    "# model.load_state_dict(torch.load('Checkpoints/RNN_3883videos_vox_6-6.pth')) # 6-6 frames\n",
    "\n",
    "# Set the model to evaluation mode (important if using dropout or batch normalization)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc109de",
   "metadata": {},
   "source": [
    "# Import keypoints of 44 VoxCeleb test videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f3df2e97",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "44"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with open(\"kp_test_44_vox.pkl\", \"rb\") as f:\n",
    "    kp_time_series = pickle.load(f)\n",
    "len(kp_time_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56134b02",
   "metadata": {},
   "source": [
    "# Convert list of keypoints to dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c95d8b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "for video_idx in range(len(kp_time_series)):\n",
    "    kp_time_series[video_idx] = kp_time_series[video_idx]['kp']\n",
    "\n",
    "kp_dict_init = []\n",
    "for video_idx in range(len(kp_time_series)): # \n",
    "    init_mean = []\n",
    "    init_jacobian = []\n",
    "    for frame_idx in range(len(kp_time_series[video_idx])):\n",
    "        kp_mean = kp_time_series[video_idx][frame_idx]['value'].reshape(1,10,2)\n",
    "        kp_mean = torch.tensor(kp_mean)\n",
    "        kp_jacobian = kp_time_series[video_idx][frame_idx]['jacobian'].reshape(1,10,2,2)\n",
    "        kp_jacobian = torch.tensor(kp_jacobian)\n",
    "\n",
    "        init_mean.append(kp_mean)\n",
    "        init_jacobian.append(kp_jacobian)\n",
    "\n",
    "    init_mean = torch.cat(init_mean)\n",
    "    init_jacobian = torch.cat(init_jacobian)\n",
    "\n",
    "    init_mean = torch.reshape(init_mean,(1,init_mean.shape[0],init_mean.shape[1],init_mean.shape[2]))\n",
    "    init_jacobian = torch.reshape(init_jacobian,(1,init_jacobian.shape[0],10,2,2))\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        # add tensor to cuda\n",
    "        init_mean = init_mean.to('cuda:0')\n",
    "        init_jacobian = init_jacobian.to('cuda:0')\n",
    "\n",
    "    kp_dict_both = {\"value\":init_mean,\"jacobian\":init_jacobian}\n",
    "    kp_dict_init.append(kp_dict_both)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67981bd7",
   "metadata": {},
   "source": [
    "# Apply min-max std to keypoints and convert to batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f4c9cfc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44\n",
      "(118, 60)\n"
     ]
    }
   ],
   "source": [
    "kp_list_test = []\n",
    "for video_idx in range(len(kp_dict_init)):\n",
    "    kp_one_video = torch.cat((kp_dict_init[video_idx]['value'], kp_dict_init[video_idx]['jacobian'].reshape(1,-1,10,4)),dim=-1).reshape(-1,60)\n",
    "    kp_one_video_array = np.array(kp_one_video.cpu())\n",
    "    kp_list_test.append(kp_one_video_array)\n",
    "    \n",
    "#####  min-max std to 60 dimensions of selected one video ######\n",
    "kp_list_test_std = []\n",
    "min_list = []\n",
    "range_list = []\n",
    "for video_idx in range(len(kp_list_test)):\n",
    "    data = kp_list_test[video_idx]\n",
    "    data_length = len(kp_list_test[video_idx])\n",
    "    step_interval = 12 # choose between 12 frames or 24 frames \n",
    "    min_required_steps = 2*step_interval\n",
    "    selected_data = []\n",
    "    for i in range(0, data_length - min_required_steps+1, 2 * step_interval):\n",
    "        selected_data.extend(data[i:i + step_interval])\n",
    "    min_values = np.min(selected_data,axis=0) # 60 mins of one selected video in the loop\n",
    "    max_values = np.max(selected_data,axis=0) # 60 maxs of one selected video in the loop \n",
    "    range_values = max_values - min_values \n",
    "    kp_one_video_std = (kp_list_test[video_idx] - min_values) / range_values\n",
    "    kp_list_test_std.append(kp_one_video_std)\n",
    "    min_list.append(min_values)\n",
    "    range_list.append(range_values)\n",
    "\n",
    "trajs = kp_list_test_std\n",
    "print(len(trajs))\n",
    "print(trajs[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0b33cd29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test dataset batches: 529\n",
      "(24, 60)\n"
     ]
    }
   ],
   "source": [
    "######### convert into batches:\n",
    "frames = min_required_steps\n",
    "input_frames = int(frames / 2)\n",
    "data_batch_test = []\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] >= frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        for arr in np.array_split(x[:num_full_batches * frames], num_full_batches):\n",
    "            data_batch_test.append(arr)\n",
    "print(f'test dataset batches:', len(data_batch_test))\n",
    "print(data_batch_test[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e7cf44c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###### test dataset:\n",
    "\n",
    "test_data_reshape = np.array(data_batch_test).reshape(-1,frames,60)\n",
    "test_data_reshape.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376c19e5-ee36-44cc-a8c7-7caff90e7d29",
   "metadata": {},
   "source": [
    "# Predict keypoints using trained model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "20c9c688-c031-4a26-8a24-2720d2b21e78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([529, 12, 60])\n",
      "torch.Size([529, 12, 60])\n"
     ]
    }
   ],
   "source": [
    "# test dataset\n",
    "validation_data = test_data_reshape\n",
    "\n",
    "# evaluate model:\n",
    "validation_input = torch.tensor(validation_data[:,:input_frames], dtype = torch.float32) # input: [24,10,17]\n",
    "kp_gt = torch.tensor(validation_data[:,input_frames:], dtype = torch.float32) # gtoundtruth: [24,10,17]\n",
    "pred = model(validation_input) # outputs: [24,10,30]\n",
    "\n",
    "print(kp_gt.shape)\n",
    "print(pred.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b19b86c4-b761-48d9-ad9a-88908129633b",
   "metadata": {},
   "source": [
    "# Generate unstd keypoints "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd71d1a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of batches of each video: 44\n"
     ]
    }
   ],
   "source": [
    "# save num_batches for each video:\n",
    "num_batch_video = []\n",
    "num_full_batches_all = 0\n",
    "for t,x in enumerate(kp_list_test_std):\n",
    "    if x.shape[0] > frames:\n",
    "        num_full_batches = x.shape[0] // frames\n",
    "        num_full_batches_all += num_full_batches\n",
    "        num_batch_video.append(num_full_batches)\n",
    "print(f'number of batches of each video:', len(num_batch_video))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dacf886e-0203-4086-bedb-4468e1cf4fd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(529, 24, 60)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# first half of frames: groundtruth; last half of frames: predicted\n",
    "test_gt_pred = np.concatenate((test_data_reshape[:,:input_frames], pred.detach().numpy()), axis = 1)\n",
    "test_gt_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e56be0d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# unstd for each video:\n",
    "test_video_unstd_list = []\n",
    "for video_idx in range(len(num_batch_video)):\n",
    "    test_video = test_gt_pred[sum(num_batch_video[:video_idx]):sum(num_batch_video[:video_idx+1])]\n",
    "    test_video_unstd = test_video * range_list[video_idx] + min_list[video_idx]\n",
    "    test_video_unstd_list.append(test_video_unstd) # unstd video keypoints"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b74a397-20c6-4395-b7b9-c14de9621770",
   "metadata": {},
   "source": [
    "# Optical flow and generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "08c8b2f0-07df-4c33-8293-af8dbd119772",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use predefined train-test split.\n",
      "using videos from test directory\n",
      "['id10280#NXjT3732Ekg#001093#001192.mp4', 'id10281#NHARUN9OhSo#000605#000886.mp4', 'id10281#NHARUN9OhSo#001059#001210.mp4', 'id10281#NHARUN9OhSo#002098#002175.mp4', 'id10281#NHARUN9OhSo#002209#002570.mp4', 'id10281#NHARUN9OhSo#006609#006906.mp4', 'id10281#NHARUN9OhSo#006912#007284.mp4', 'id10281#NHARUN9OhSo#007425#007663.mp4', 'id10282#IDA_ElNHLn4#000674#000852.mp4', 'id10282#IDA_ElNHLn4#001226#001390.mp4', 'id10283#N69Hp2DGMLk#000519#000619.mp4', 'id10283#N69Hp2DGMLk#000721#000842.mp4', 'id10283#N69Hp2DGMLk#000893#001589.mp4', 'id10283#N69Hp2DGMLk#004133#005157.mp4', 'id10283#N69Hp2DGMLk#005157#005316.mp4', 'id10283#N69Hp2DGMLk#005931#006184.mp4', 'id10283#N69Hp2DGMLk#006184#006353.mp4', 'id10283#N69Hp2DGMLk#006405#006583.mp4', 'id10283#N69Hp2DGMLk#006600#007118.mp4', 'id10283#N69Hp2DGMLk#007129#007281.mp4', 'id10283#r9-0pljhZqs#002414#002769.mp4', 'id10283#r9-0pljhZqs#003725#003847.mp4', 'id10283#r9-0pljhZqs#004062#004408.mp4', 'id10283#r9-0pljhZqs#007271#007498.mp4', 'id10283#r9-0pljhZqs#007920#008172.mp4', 'id10283#r9-0pljhZqs#008373#008488.mp4', 'id10283#r9-0pljhZqs#009636#009808.mp4', 'id10283#r9-0pljhZqs#010121#010605.mp4', 'id10283#r9-0pljhZqs#010765#010954.mp4', 'id10283#r9-0pljhZqs#011158#011299.mp4', 'id10283#r9-0pljhZqs#012307#012561.mp4', 'id10283#r9-0pljhZqs#013264#013592.mp4', 'id10283#r9-0pljhZqs#013743#013963.mp4', 'id10283#r9-0pljhZqs#014353#014609.mp4', 'id10283#r9-0pljhZqs#014722#014831.mp4', 'id10283#r9-0pljhZqs#014864#015079.mp4', 'id10283#r9-0pljhZqs#015364#015539.mp4', 'id10283#rUnfAxF1dJI#001389#001843.mp4', 'id10283#uA1E_38TuTw#001699#001863.mp4', 'id10283#uA1E_38TuTw#002329#002480.mp4', 'id10283#uA1E_38TuTw#002875#002990.mp4', 'id10283#uA1E_38TuTw#003095#003227.mp4', 'id10283#uA1E_38TuTw#003246#003371.mp4', 'id10283#x6M3KQ8-gM4#000018#000441.mp4']\n",
      "44\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jovyan/gen-mod-vol/tristen/Motion-Transfer-Keypoints-Prediction/Keypoints_Prediction/vmaf/libvmaf/.venv/lib/python3.11/site-packages/torch/functional.py:512: UserWarning: torch.meshgrid: in an upcoming release, it will be required to pass the indexing argument. (Triggered internally at ../aten/src/ATen/native/TensorShape.cpp:3587.)\n",
      "  return _VF.meshgrid(tensors, **kwargs)  # type: ignore[attr-defined]\n"
     ]
    }
   ],
   "source": [
    "####### call the config functions and inference dataloader #########\n",
    "config=\"./config/abs-vox.yml\"\n",
    "\n",
    "# Test dataset\n",
    "with open(config) as f:\n",
    "    config = yaml.safe_load(f)\n",
    "dataset = FramesDataset(is_train=(False), **config['dataset_params'],mode=\"RNN\") # test\n",
    "\n",
    "print(len(dataset))\n",
    "dataloader = DataLoader(dataset, batch_size=1, shuffle=False, num_workers=1)\n",
    "\n",
    "### call the functions        \n",
    "generator = OcclusionAwareGenerator(**config['model_params']['generator_params'],\n",
    "                                        **config['model_params']['common_params'])\n",
    "kp_detector = KPDetector(**config['model_params']['kp_detector_params'],\n",
    "                            **config['model_params']['common_params'])\n",
    "\n",
    "log_dir=\"./log/test-reconstruction-vox\"\n",
    "checkpoint=\"./Training_Prediction/FOMM/Trained_Models/vox-cpk.pth.tar\"\n",
    "\n",
    "if checkpoint is not None:\n",
    "    Logger.load_cpk(checkpoint, generator=generator, kp_detector=kp_detector)\n",
    "else:\n",
    "    raise AttributeError(\"Checkpoint should be specified for mode='reconstruction'.\")\n",
    "    \n",
    "def save_obj(obj, name ):\n",
    "    with open('./'+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('./' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "        \n",
    "png_dir = os.path.join(log_dir, 'prediction/png')\n",
    "log_dir = os.path.join(log_dir, 'prediction')\n",
    "\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "\n",
    "if not os.path.exists(png_dir):\n",
    "    os.makedirs(png_dir)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    generator = DataParallelWithCallback(generator)\n",
    "    kp_detector = DataParallelWithCallback(kp_detector)\n",
    "\n",
    "generator.eval()\n",
    "kp_detector.eval()\n",
    "\n",
    "prediction_params = config['prediction_params']\n",
    "\n",
    "num_epochs = prediction_params['num_epochs']\n",
    "lr = prediction_params['lr']\n",
    "bs = prediction_params['batch_size']\n",
    "num_frames = prediction_params['num_frames']\n",
    "loss_list_total = []\n",
    "fvd_list_total = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "09a2bd38",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]/home/jovyan/gen-mod-vol/tristen/Motion-Transfer-Keypoints-Prediction/Keypoints_Prediction/vmaf/libvmaf/.venv/lib/python3.11/site-packages/torch/nn/functional.py:4343: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n",
      "1it [00:03,  3.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.069955476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2it [00:13,  7.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.05067924\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3it [00:18,  6.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.061348695\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4it [00:21,  4.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.06758981\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "5it [00:34,  7.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.07394007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "6it [00:44,  8.46s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.08074589\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "7it [00:57,  9.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.05999605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8it [01:04,  9.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.054235034\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9it [01:09,  7.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.10292894\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10it [01:14,  6.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.07873031\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "11it [01:18,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.042145252\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "12it [01:22,  5.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.044186223\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "13it [01:47, 11.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.052939765\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "14it [02:24, 18.94s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.051838864\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15it [02:29, 14.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.055624884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "16it [02:37, 12.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.0676938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "17it [02:43, 10.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.060428556\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "18it [02:49,  9.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.052590877\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "19it [03:07, 11.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.0648884\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "20it [03:12,  9.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.06157716\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "21it [03:25, 10.70s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.071538426\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "22it [03:29,  8.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.058248665\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "23it [03:41,  9.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.07494083\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24it [03:49,  9.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.06661647\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25it [03:58,  9.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.07490342\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "26it [04:02,  7.49s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.06279347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "27it [04:07,  6.98s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.062716626\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "28it [04:24,  9.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.06446099\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "29it [04:31,  8.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.06397948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "30it [04:36,  7.74s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.07525618\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "31it [04:44,  7.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.06610256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32it [04:56,  9.14s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.066081755\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "33it [05:03,  8.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.067728676\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "34it [05:12,  8.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.08904547\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "35it [05:16,  7.05s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.08821155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "36it [05:23,  7.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.09193583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "37it [05:29,  6.75s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.059987795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "38it [05:44,  9.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.043185733\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "39it [05:50,  8.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.060258795\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "40it [05:55,  7.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.04956418\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41it [05:58,  6.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.0762435\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "42it [06:03,  5.64s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.059528988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "43it [06:07,  5.24s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.050037395\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "44it [06:23,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reconstruction loss: 0.043088377\n",
      "mean Reconstruction loss: 0.06455724\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "#########  FOMM+RNN ########\n",
    "\n",
    "for it, x in tqdm(enumerate(dataloader)):\n",
    "        if config['reconstruction_params']['num_videos'] is not None:\n",
    "            if it > config['reconstruction_params']['num_videos']:\n",
    "                break\n",
    "        with torch.no_grad():\n",
    "            predictions = []\n",
    "            visualizations = []\n",
    "\n",
    "            ######## keypoints ########\n",
    "            kp_driving_video = test_video_unstd_list[it].reshape(-1,10,6)\n",
    "            kp_driving_video = torch.tensor(kp_driving_video)\n",
    "            kp_source = {\"value\":kp_driving_video[0,:,:2].reshape(1,10,2),\"jacobian\":kp_driving_video[0,:,2:].reshape(1,10,2,2)} # kp of the ith frame      \n",
    "        \n",
    "        ##### Start generator\n",
    "        loss_list = []\n",
    "        fvd_list = []\n",
    "        for i in range(((x['video'].shape[2])//frames)*frames): # cut the last <24 frames\n",
    "            source = x['video'][:, :, 0]\n",
    "            driving = x['video'][:, :, i]\n",
    "            kp_driving = {\"value\":kp_driving_video[i,:,:2],\"jacobian\":kp_driving_video[i,:,2:]} # kp of the ith frame\n",
    "            kp_driving['value'] = kp_driving['value'].reshape(1,10,2)\n",
    "            kp_driving['jacobian'] = kp_driving['jacobian'].reshape(1,10,2,2)\n",
    "            out = generator(source, kp_source=kp_source, kp_driving=kp_driving)\n",
    "            out['kp_source'] = kp_source\n",
    "            out['kp_driving'] = kp_driving\n",
    "            del out['sparse_deformed']\n",
    "            predictions.append(np.transpose(out['prediction'].data.cpu().numpy(), [0, 2, 3, 1])[0])\n",
    "\n",
    "            visualization = Visualizer(**config['visualizer_params']).visualize(source=source,\n",
    "                                                                                    driving=driving, out=out)\n",
    "            visualizations.append(visualization)\n",
    "            # mse loss\n",
    "            if np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean() != 0:\n",
    "                loss_list.append(np.abs(out['prediction'].detach().cpu().numpy() - driving.cpu().numpy()).mean())\n",
    "                # # Calculate FVD for each frame using ground truth and predicted videos\n",
    "                # ground_truth_features = driving.detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "                # predicted_features = out['prediction'].detach().cpu().permute(0,2,3,1).reshape(256,256,3)\n",
    "                # fvd_list.append(compute_fvd(ground_truth_features, predicted_features))\n",
    "\n",
    "        print(\"Reconstruction loss: %s\" % np.mean(loss_list))\n",
    "        loss_list_total.append(np.mean(loss_list))\n",
    "\n",
    "        # print(\"FVD Score: %s\" % np.mean(fvd_list))\n",
    "        # fvd_list_total.append(np.mean(fvd_list))\n",
    "\n",
    "#        predictions = np.concatenate(predictions, axis=1)\n",
    "#        imageio.imsave(os.path.join(png_dir, x['name'][0] + '.png'), (255 * predictions).astype(np.uint8))\n",
    "#        image_name = x['name'][0] + config['reconstruction_params']['format']\n",
    "#        imageio.mimsave(os.path.join(log_dir, image_name), visualizations)\n",
    "\n",
    "print(\"mean Reconstruction loss: %s\" % np.mean(loss_list_total)) \n",
    "# print(\"mean FVD score: %s\" % np.mean(fvd_list_total)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
